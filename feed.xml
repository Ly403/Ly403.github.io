<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://ly403.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ly403.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-16T11:52:12+00:00</updated><id>https://ly403.github.io/feed.xml</id><title type="html">Yi Liu’s Homepage. 😀</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">离散型流匹配模型（3）</title><link href="https://ly403.github.io/blog/2025/%E7%A6%BB%E6%95%A3%E6%B5%81%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-3/" rel="alternate" type="text/html" title="离散型流匹配模型（3）"/><published>2025-01-20T00:00:00+00:00</published><updated>2025-01-20T00:00:00+00:00</updated><id>https://ly403.github.io/blog/2025/%E7%A6%BB%E6%95%A3%E6%B5%81%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B%EF%BC%883%EF%BC%89</id><content type="html" xml:base="https://ly403.github.io/blog/2025/%E7%A6%BB%E6%95%A3%E6%B5%81%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-3/"><![CDATA[<p>接着上次说的DFM的分解技巧，继续学习DFM的最后一部分——混合路径（Mixture paths）。</p> <h2 id="混合路径mixture-paths">混合路径（Mixture paths）</h2> <p>我们在此前的文章里面已经讲解了分解技巧<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>，得到了分解的（边缘/条件）概率路径、分解的（边缘/条件）速率，以及知道了他们之间的关系（<sup id="fnref:1:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>中的命题2和定理16）。现在我们要着手设计出一个具体的DFM了，类似FM<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>一样，<strong>我们可以从分解条件概率路径着手，然后得到目标分解条件速率，使用神经网络预测分解条件速率，最后使用分解CDFM损失（<sup id="fnref:1:2"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>中公式14）来训练模型。</strong></p> <p>我们以$Z=(X_0,X_1) \sim \pi_{0,1}(X_0,X_1)$为条件（$X_0$和$X_1$为任意配对），那么分解的条件概率路径可以使用如下形式：</p> \[p_{t\mid 0,1}^i (x^i\mid x_0,x_1)=\kappa_t\delta(x^i,x_1^i) + (1-\kappa_t)\delta(x^i,x_0^i) \tag{1}\] <p>这个做法类似Rectified Flow（<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>中公式1），只是使用了更general的$\kappa_t$作为系数，$\kappa : [0,1] \to [0,1]$就类似扩散模型里面的噪声调度器，$\kappa_t \in C^1([0,1])$。根据<sup id="fnref:1:3"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>中公式6，我们能得到非分解的条件概率路径：</p> \[p_{t\mid 0,1} (x\mid x_0,x_1)=\prod_i p_{t\mid 0,1}^i (x^i\mid x_0,x_1) \tag{2}\] <blockquote> <p>关于公式1的详细解释：</p> <p>分解概率路径上的随机变量$X_t^i \sim p_{t\mid 0,1}^i(\cdot\mid x_0,x_1) $满足：</p> \[X_t^i = \begin{cases} x_1^i \quad 概率为\kappa_t\\ x_0^i \quad 概率为(1-\kappa_t) \end{cases} \tag{3}\] <p>也就是对$t$时刻的第$i$个token $X_t^i$，它要么取源状态的值$x_0^i$，要么取目标状态的值$x_1^i$，取这两个值的概率与时间$t$相关。</p> </blockquote> <p>从公式1和2给出的条件概率路径可以算出边缘概率路径，即：</p> \[p_t(x)=\sum_{x_0,x_1}p_{t\mid 0,1}(x\mid x_0,x_1) p_{0,1}(x_0,x_1) \tag{4}\] <p>需要指出的是，<strong>$p_t(x)$需要满足边缘条件，即$p_0(x) = \delta(x,x_0)$，$p_1(x) = \delta(x,x_1)$。为实现这个条件，需要让$\kappa_0 = 0$，$\kappa_1 = 1$。</strong></p> <p>接下来，我们需要知道公式1所示分解条件概率路径对应的分解条件速率$u_t^i(y^i,x^i\mid x_0,x_1)$。由Kolmogorov方程可知：</p> \[\cfrac{\mathrm{d}}{\mathrm{d}t}p_{t\mid 0,1}^i(y^i\mid x_0,x_1) = \sum_{x^i}u_t^i(y^i,x^i\mid x_0,x_1)p_{t\mid 0,1}^i(x^i\mid x_0,x_1) \tag{5}\] <p>结合公式1给出的具体分解条件概率路径，可以算出：</p> \[\begin{aligned} \cfrac{\mathrm{d}}{\mathrm{d}t}p_{t\mid 0,1}^i(y^i\mid x_0,x_1)&amp; \overset{公式1}{=} \dot \kappa_t\left[\delta(y^i,x_1^i) - \delta(y^i,x_0^i)\right] \\ &amp;\overset{用公式1消去\delta(y^i,x_0^i)}{=} \dot \kappa_t\left[\delta(y^i,x_1^i) - \cfrac{p^i_{t\mid 0,1}(y^i\mid x_0,x_1)-\kappa_t\delta(y^i,x_1^i)}{1-\kappa_t}\right]\\ &amp;{=} \cfrac{\dot \kappa_t}{1 - \kappa_t}\left[({1-\kappa_t})\delta(y^i,x_1^i) - {p^i_{t\mid 0,1}(y^i\mid x_0,x_1)+\kappa_t\delta(y^i,x_1^i)}\right]\\ &amp;{=} \cfrac{\dot \kappa_t}{1 - \kappa_t} \left[ \delta(y^i,x_1^i) - p^i_{t\mid 0,1}(y^i\mid x_0,x_1) \right]\\ &amp;\overset{(*)}{=}\sum_{x^i} \cfrac{\dot \kappa_t}{1 - \kappa_t} \left[ \delta(y^i,x_1^i) - \delta(y^i,x^i) \right] p^i_{t\mid 0,1}(x^i\mid x_0,x_1)\\ \end{aligned}\tag{6}\] <blockquote> <p>关于（*）步的解释如下：</p> <p>这里其实就是要证明</p> \[\delta(y^i,x_1^i) - p^i_{t\mid 0,1}(y^i\mid x_0,x_1) = \sum_{x^i} \left[ \delta(y^i,x_1^i) - \delta(y^i,x^i) \right] p^i_{t\mid 0,1}(x^i\mid x_0,x_1) \tag{7}\] <p>我们从右边开始变换：</p> \[\begin{aligned} \sum_{x^i} \left[ \delta(y^i,x_1^i) - \delta(y^i,x^i) \right] p^i_{t\mid 0,1}(x^i\mid x_0,x_1) &amp;= \sum_{x^i} \left[ \delta(y^i,x_1^i)p^i_{t\mid 0,1}(x^i\mid x_0,x_1) - \delta(y^i,x^i)p^i_{t\mid 0,1}(x^i\mid x_0,x_1) \right]\\ &amp;= \sum_{x^i} \delta(y^i,x_1^i)p^i_{t\mid 0,1}(x^i\mid x_0,x_1) - \sum_{x^i} \delta(y^i,x^i)p^i_{t\mid 0,1}(x^i\mid x_0,x_1) \\ &amp;\overset{(1)}{=} \sum_{x^i} \delta(y^i,x_1^i)p^i_{t\mid 0,1}(x^i\mid x_0,x_1) - p^i_{t\mid 0,1}(y^i\mid x_0,x_1) \\ &amp;\overset{(2)}{=} \delta(y^i,x_1^i) - p^i_{t\mid 0,1}(y^i\mid x_0,x_1) \\ \end{aligned} \tag{8}\] <p>第（1）步是delta函数的性质，$\int_x\delta(y,x)f(x)\mathrm{d}x = f(y)$，第（2）步是因为$\delta(y^i,x_1^i)$与$x^i$无关，$\sum_{x^i} p^i_{t\mid 0,1}(x^i\mid x_0,x_1) = 1 $。</p> </blockquote> <p>对比公式6的最后结果和公式5，就知道：</p> \[u_t^i(y^i,x^i\mid x_0,x_1) = \cfrac{\dot \kappa_t}{1 - \kappa_t} \left[ \delta(y^i,x_1^i) - \delta(y^i,x^i) \right] \tag{9}\] <p>这样我们就知道了公式1所示的分解条件概率路径对应的分解条件速率$u_t^i(y^i,x^i\mid x_0,x_1)$的具体形式了。</p> <p>Code 9是混合路径的实现代码。</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post/DFM31-480.webp 480w,/assets/img/post/DFM31-800.webp 800w,/assets/img/post/DFM31-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post/DFM31.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 混合路径的实现代码 </div> <h3 id="速率后验参数化">速率后验参数化</h3> <p>类比我们在FM<sup id="fnref:2:1"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>和扩散模型里面也会讲的多种预测方式，例如velocity-prediction、$x_0$-prediction、$x_1$-prediction，我们在DFM里面也可以使用类似的预测方式。</p> <p>最简单的一种：<strong>使用神经网络预测公式9给出的分解条件速率$u_t^i(y^i,x^i\mid x_0,x_1)$，这就是velocity-prediction。</strong></p> <p>接下来介绍DFM里面的$x_1$-prediction，这个会稍微复杂一点。</p> <p>根据我们在<sup id="fnref:1:4"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>中证明出来的公式12，可以得到在$u_t^i(y^i,x^i\mid x_0,x_1)$取公式9的形式时，分解边缘速率为：</p> \[\begin{aligned} u_t^i(y^i,x) &amp;=\sum_{x_0,x_1} u_t^i(y^i,x^i\mid x_0,x_1)p_{0,1\mid t}(x_0,x_1\mid x)\\ &amp;\overset{公式(9)}{=} \sum_{x_0,x_1} \cfrac{\dot \kappa_t}{1 - \kappa_t} \left[ \delta(y^i,x_1^i) - \delta(y^i,x^i) \right] p_{0,1\mid t}(x_0,x_1\mid x)\\ &amp;\overset{将x_1^i分出来}{=} \sum_{x_1^i}\sum_{x_0,x_1^{\bar i}} \cfrac{\dot \kappa_t}{1 - \kappa_t} \left[ \delta(y^i,x_1^i) - \delta(y^i,x^i) \right] p_{0,1\mid t}(x_0,x_1\mid x)\\ &amp;\overset{分配律}{=} \sum_{x_1^i} \cfrac{\dot \kappa_t}{1 - \kappa_t} \left[ \delta(y^i,x_1^i) - \delta(y^i,x^i) \right]\sum_{x_0,x_1^{\bar i}} p_{0,1\mid t}(x_0,x_1\mid x)\\ \end{aligned} \tag{10}\] <p>单独把第二个求和式拿出来，记为：</p> \[p_{1\mid t}^i(x_1^i\mid x) = \sum_{x_0,x_1^{\bar i}} p_{0,1\mid t}(x_0,x_1\mid x) \overset{(*)}{=}\mathbb{E}\left[ \delta(x_1^i,X_1^i) \mid X_t = x \right] \tag{11}\] <blockquote> <p>关于公式11中（*）步骤的详细说明：</p> \[\begin{aligned} p_{1\mid t}^i(x_1^i\mid x) &amp;= \sum_{x_0,x_1^{\bar i}} p_{0,1\mid t}(x_0,x_1\mid x)\\ &amp;\overset{拆开x_1}{=} \sum_{x_0,x_1^{\bar i}} p_{0,1\mid t}(x_0,x_1^{\bar i},x_1^i\mid x)\\ &amp;\overset{\delta函数的性质}{=} \sum_{x_0,x_1^{\bar i}}\sum_{X_1^i}\delta(x_1^i,X_1^i) p_{0,1\mid t}(x_0,x_1^{\bar i},X_1^i\mid x)\\ &amp;=\mathbb{E}\left[ \delta(x_1^i,X_1^i) \mid X_t = x \right] \end{aligned} \tag{12}\] </blockquote> <p>我们可以用神经网络去学习后验分布$p_{1\mid t}^{\theta,i}(x_1^i\mid x)$，$\theta$为神经网络的参数。这就是离散版的$x_1$-prediction。</p> <p>这样理解：以文本为例，<strong>$x$是时间$t$时的“加噪”文本（但其实说“加噪“不准确，因为DFM的初始状态$x_0$是一个全mask的文本，$x$更类似一个处于中间状态的文本）。$x$作为神经网络的输入，神经网络预测$x_1^i$，$i\in [d]$表示这是$x_1$的第$i$个token，$d$为句长。神经网络一次预测所有token的单步变化概率，每个token共有$K$种变化情况（$K$为vocabulary的大小），神经网络的输出向量的维度就是$d\cdot K$。</strong></p> <h3 id="混合路径的cdfm损失">混合路径的CDFM损失</h3> <p>我们基于$x_1$-prediction，即预测$p_{1\mid t}^{\theta,i}(x_1^i\mid x)$，继续讨论公式1所示混合路径情况下的CDFM损失。这里会有两种做法。</p> <p>结合公式11，如果直接对比后验概率，则CDFM损失可以写为：</p> \[L_{CDFM}(\theta) = \mathbb{E}_{t,X_0,X_1,X_t} D_{X_t}\left( \delta(\cdot,X_1^i),p_{1\mid t}^{\theta,i}(\cdot\mid X_t) \right) \tag{13}\] <p>$\delta(\cdot,X_1^i)$，$p_{1\mid t}^{\theta,i}(\cdot\mid X_t)$都是概率质量函数，所以可以使用KL散度衡量二者之间的距离，即取Bregman散度为KL散度，得到：</p> \[L_{CDFM}(\theta) = - \mathbb{E}_{t,X_0,X_1,X_t} \log p_{1\mid t}^{\theta,i}(X_1^i\mid X_t) + \text{const} \tag{14}\] <p>这是第一种做法。</p> <blockquote> <p>关于公式14的详细说明：</p> <p>KL散度的公式是：$D(p,q)=\sum_{\alpha \in \mathcal{T}}p(\alpha)\log\cfrac{p(\alpha)}{q(\alpha)}$。在选择KL散度为Bregman散度后，公式13可以化为：</p> \[\begin{aligned} L_{CDFM}(\theta) &amp;= \mathbb{E}_{t,X_0,X_1,X_t} \sum_{X_1^i} \delta(x_1^i,X_1^i) \log\cfrac{\delta(x_1^i,X_1^i)}{p_{1\mid t}^{\theta,i}( X_1^i\mid X_t)}\\ &amp;=\mathbb{E}_{t,X_0,X_1,X_t} \sum_{X_1^i}\left[ \delta(x_1^i,X_1^i) \log{\delta(x_1^i,X_1^i)} - \delta(x_1^i,X_1^i) \log{p_{1\mid t}^{\theta,i}( X_1^i\mid X_t)}\right]\\ &amp;= \text{const} - \mathbb{E}_{t,X_0,X_1,X_t} \sum_{X_1^i}\delta(x_1^i,X_1^i) \log{p_{1\mid t}^{\theta,i}( X_1^i\mid X_t)}\\ &amp;\overset{\delta函数的性质}{=} \text{const} - \mathbb{E}_{t,X_0,X_1,X_t} \log{p_{1\mid t}^{\theta,i}(x_1^i\mid X_t)} \end{aligned} \tag{15}\] </blockquote> <p>第二种做法是使用<sup id="fnref:1:5"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>中公式14所示的分解CDFM损失，如下：</p> \[L_{CDFM}(\theta) = \mathbb{E}_{t,Z,X_t\sim p_{t\mid Z}} \sum_i D_{X_t}^i \left( u_t^i(\cdot,X_t \mid Z), u_t^{\theta,i}(\cdot, X_t) \right) \tag{16}\] <p>取$Z=(X_0,X_1)$。使用该公式需要知道$u_t^{\theta,i}$，因为神经网络预测了$p_{1\mid t}^{\theta,i}$，所以$u_t^{\theta,i}$是能算出来的。</p> <p>公式16中的Bregman散度可以选择广义的KL损失（其输入不一定要是概率分布）。具体而言，对于向量$u,v\in \mathbb R_{\ge 0}^{m}$，广义KL损失是：</p> \[D(u,v) = \sum_j u^j \log \cfrac{u^j}{v^j} - \sum_j u_j + \sum_jv_j \tag{17}\] <p>这种选择下对应的Bregman散度为：</p> \[D\left(u_t^i(\cdot,x^i\mid x_0,x_1),u^{i,\theta}_t(\cdot,x)\right)= \cfrac{\dot \kappa_t}{1-\kappa_t}\left[ \left(\delta(x_1^i,x^i) - 1\right)\log p_{1\mid t}^{i,\theta}(x_1^i\mid x) + \delta(x_1^i,x^i)- p_{1\mid t}^{i,\theta}(x^i\mid x) \right] \tag{18}\] <h3 id="混合轨迹采样">混合轨迹采样</h3> <p>我们在<sup id="fnref:1:6"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>中的公式5已经给出per coordinate的采样方式，结合前面讨论的混合路径，采样公式可以写为：</p> \[\mathbb{P}(X^i_{t+h}=y^i\mid X_t = x) =\delta(y^i,x^i) + h u_t^i(y^i,x) +o(h) \tag{19}\] <p>在混合路径的特定情况下为：</p> \[\begin{aligned} \mathbb{P}(X^i_{t+h}=y^i\mid X_t = x) &amp;\overset{公式10}{=} \delta(y^i,x^i) + o(h) + h \sum_{x_1^i} \cfrac{\dot \kappa_t}{1 - \kappa_t} \left[ \delta(y^i,x_1^i) - \delta(y^i,x^i) \right]\sum_{x_0,x_1^{\bar i}} p_{0,1\mid t}(x_0,x_1\mid x)\\ &amp;\overset{公式11}{=} \delta(y^i,x^i) + o(h) + h \sum_{x_1^i} \cfrac{\dot \kappa_t}{1 - \kappa_t} \left[ \delta(y^i,x_1^i) - \delta(y^i,x^i) \right] p_{1\mid t}^i(x_1^i\mid x)\\ &amp;\overset{\sum_{x_1^i}p_{1\mid t}^i(x_1^i\mid x)=1 }{=} \sum_{x_1^i}\left[ \delta(y^i,x^i) + h \cfrac{\dot \kappa_t}{1 - \kappa_t} \left[ \delta(y^i,x_1^i) - \delta(y^i,x^i) \right] + o(h) \right] p_{1\mid t}^i(x_1^i\mid x)\\ \end{aligned} \tag{20}\] <p>在已知$X_t = x$的情况下，使用公式20进行采样的具体步骤如下：</p> <ol> <li>从$p_{1\mid t}^i(X_1^i\mid x)$采样出$X_1^i$，$X_1^i\sim p_{1\mid t}^i(X_1^i\mid x)$。$p_{1\mid t}^i(X_1^i\mid x)$是由模型预测得到。</li> <li>基于$X_t^i$更新出$X_{t+h}^i$。这一步需要使用<sup id="fnref:5"><a href="#fn:5" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>中公式9所示的Euler step，并将其中的速率设置为$\cfrac{\dot \kappa_t}{1 -\kappa_t}\left[\delta(y^i,X_1^i) - \delta(y^i,x^i)\right]$。<strong>这一步决定了$X_{t+h}^i=X_t^i$还是$X_{t+h}^i=X_1^i$</strong>。</li> </ol> <h3 id="单边混合路径和保概率速率">单边混合路径和保概率速率</h3> <p>我们在第一次讲CTMC的那篇文章<sup id="fnref:5:1"><a href="#fn:5" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>里面的最后一部分讲了一下保概率速率，到这里终于要用到了。</p> <p>简单回顾一下在CTMC模型里面讲到的保概率速率，它简单的说就是<strong>在已有的速率$u_t(y,x)$中加入另一个速率$v_t(y,x)$，只要$v_t(y,x)$是divergence-free的，那么$u_t(y,x)$生成概率路径不会改变</strong>。所谓divergence-free，指$\sum_x v_t(y,x)p_t(x) = 0$。</p> <p>对DFM里面的分解条件概率路径$p_{t\mid Z}^i(x^i\mid z)$，其对应的保概率条件速率$v_t^i(y^i,x^i\mid z)$满足：</p> \[\sum_{x^i} v_t^i(y^i,x^i\mid z)p_{t\mid Z}^i(x^i\mid z) = 0 \tag{21}\] <p>一般来说这个保概率条件速率$v_t^i(y^i,x^i\mid z)$不是特别好找到。但是如果我们能有如下两个假设：</p> <ol> <li>iid的目标分布： $p(x) = \prod_i p(x^i)$。</li> <li>源分布和目标分布的数据独立配对：$\pi_{0,1}(x_0,x_1) = p(x_0)q(x_1)$。</li> </ol> <p>那么保概率条件速率的解析式还是比较好找到的。接下来我们来构造这个保概率条件速率。</p> <p>根据前面的两条假设，能写出混合路径的形式如下：</p> \[p_t(x) = \sum_{x_1} p_{t\mid 1}(x\mid x_1)q(x_1) \text{ where } p_{t\mid 1} (x) = \prod_i p_{t\mid 1}^i(x^i \mid x_1)\\ p_{t\mid 1}^i(x^i \mid x_1) = \kappa_t \delta(x^i,x_1^i) + (1 - \kappa_t)p(x^i) \tag{22}\] <p>这个写法和我们前面的混合概率路径的写法的差异就是只用$x_1$作为条件，但其实它和用$x_0,x_1$都作为条件应该是等价的。</p> <p>类比公式6</p> \[\begin{aligned} \cfrac{\mathrm{d}}{\mathrm{d}t}p_{t\mid 1}^i(y^i\mid x_1) &amp; \overset{公式22}{=} \dot \kappa_t\left[\delta(y^i,x_1^i) - p(x^i)\right] \\ &amp;\overset{用公式22消去p(x^i)}{=} \dot \kappa_t\left[\delta(y^i,x_1^i) - \cfrac{p^i_{t\mid 1}(y^i\mid x_1)-\kappa_t\delta(y^i,x_1^i)}{1-\kappa_t}\right]\\ &amp;{=} \cfrac{\dot \kappa_t}{1 - \kappa_t}\left[({1-\kappa_t})\delta(y^i,x_1^i) - {p^i_{t\mid 1}(y^i\mid x_1)+\kappa_t\delta(y^i,x_1^i)}\right]\\ &amp;{=} \cfrac{\dot \kappa_t}{1 - \kappa_t} \left[ \delta(y^i,x_1^i) - p^i_{t\mid 1}(y^i\mid x_1) \right]\\ &amp;{=}\sum_{x^i} \cfrac{\dot \kappa_t}{1 - \kappa_t} \left[ \delta(y^i,x_1^i) - \delta(y^i,x^i) \right] p^i_{t\mid 1}(x^i\mid x_1)\\ \end{aligned}\tag{23}\] <p>对比Kolmogorov方程，知道：</p> \[u_t^i(y^i,x^i\mid x_1)=\cfrac{\dot \kappa_t}{1 - \kappa_t} \left[ \delta(y^i,x_1^i) - \delta(y^i,x^i) \right] \tag{24}\] <p>可见$u_t^i(y^i,x^i\mid x_1)$生成$p^i_{t\mid 1}(x^i\mid x_1)$。如果要找到对$p^i_{t\mid 1}(x^i\mid x_1)$而言divergence-free的条件速率，一个简单的技巧是<strong>找到一个和$u_t^i(y^i,x^i\mid x_1)$方向相反的条件速率<sup id="fnref:6"><a href="#fn:6" class="footnote" rel="footnote" role="doc-noteref">5</a></sup>，这样在总时间范围内折两个速率差的散度和就是0，所以这两速率的差就是我们要找的divergence-free的条件速率，也就是保概率条件速率。</strong>记这个反向的条件速率为$\tilde u_t^i(y^i,x^i\mid x_1)$，它的形式应该是：</p> \[\tilde u_t^i(y^i,x^i\mid x_1) = \cfrac{\dot \kappa_t}{\kappa_t} \left[ \delta(y^i,x^i) - p(x^i) \right] \tag{25}\] <p>证明在<sup id="fnref:6:1"><a href="#fn:6" class="footnote" rel="footnote" role="doc-noteref">5</a></sup>的E.5和E.6中。</p> <p>所以divergence-free的速率是：</p> \[v_t^i(y^i,x^i \mid x_1) =u_t^i(y^i,x^i\mid x_1) -\tilde u_t^i(y^i,x^i\mid x_1)\tag{26}\] <p><strong>因为$v_t^i(y^i,x^i \mid x_1)$是divergence-free的，所以把它以任意比例加到原来的速率里面都不会影响生成的概率路径</strong>。所以公式10所示分解边缘速率如果改写成下面的形式：</p> \[\begin{aligned} u_t^i(y^i,x) &amp;=\sum_{x_1} \left[ u_t^i(y^i,x^i\mid x_1) + c_t v_t^i(y^i,x^i\mid x_1) \right]p_{1\mid t}(x_1 \mid x)\\ &amp;=\sum_{x_1^i} \left[ u_t^i(y^i,x^i\mid x_1^i) + c_t v_t^i(y^i,x^i\mid x_1^i) \right]p^i_{1\mid t}(x^i_1 \mid x)\\ \end{aligned} \tag{27}\\\] <p>不影响所生成的边缘概率路径。第二个等式是因为$u_t^i(y^i,x^i\mid x_1) =u_t^i(y^i,x^i\mid x_1^i)$，$v_t^i(y^i,x^i\mid x_1) =v_t^i(y^i,x^i\mid x_1^i)$。</p> <p>那么，用公式27所示的速率进行采样的步骤是：</p> <ol> <li> <p>从$p_{1\mid t}^i(X_1^i\mid x)$采样出$X_1^i$，$X_1^i\sim p_{1\mid t}^i(X_1^i\mid x)$。$p_{1\mid t}^i(X_1^i\mid x)$是由模型预测得到。</p> </li> <li> <p>基于$X_t^i$更新出$X_{t+h}^i$。这一步需要使用<sup id="fnref:5:2"><a href="#fn:5" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>中公式9所示的Euler step，并将其中的速率设置为</p> \[u_t^i(y^i,x^i\mid x_1) = \cfrac{\dot \kappa_t}{1-\kappa_t}\left[\delta(y^i,X_1^i) - \delta(y^i,x^i)\right] +c_t\left[ \cfrac{\dot \kappa_t}{1-\kappa_t}\left[\delta(y^i,x_1^i) - \delta(y^i,x^i)\right] - \cfrac{\dot \kappa_t}{\kappa_t} \left[\delta(y^i,x^i) - p(x^i)\right] \right] \tag{28}\] <p>其中$c_t&gt;0$是与时间相关的常数。</p> </li> </ol> <p>在<a href="https://github.com/facebookresearch/flow_matching">https://github.com/facebookresearch/flow_matching</a>库里面实现的DFM的代码如下：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>  

<span class="kn">from</span> <span class="n">flow_matching.path</span> <span class="kn">import</span> <span class="n">MixtureDiscreteProbPath</span><span class="p">,</span> <span class="n">DiscretePathSample</span>  
<span class="kn">from</span> <span class="n">flow_matching.path.scheduler</span> <span class="kn">import</span> <span class="n">PolynomialConvexScheduler</span>  
<span class="kn">from</span> <span class="n">flow_matching.loss</span> <span class="kn">import</span> <span class="n">MixturePathGeneralizedKL</span>  
<span class="kn">from</span> <span class="n">flow_matching.solver</span> <span class="kn">import</span> <span class="n">MixtureDiscreteEulerSolver</span>  
<span class="kn">from</span> <span class="n">flow_matching.utils</span> <span class="kn">import</span> <span class="n">ModelWrapper</span>  

<span class="c1"># Define a trainable velocity model  
</span><span class="n">model</span> <span class="o">=</span> <span class="p">...</span>   

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span>  

<span class="n">scheduler</span> <span class="o">=</span> <span class="nc">PolynomialConvexScheduler</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>  
<span class="n">path</span> <span class="o">=</span> <span class="nc">MixtureDiscreteProbPath</span><span class="p">(</span><span class="n">scheduler</span><span class="o">=</span><span class="n">scheduler</span><span class="p">)</span>  
<span class="n">loss_fn</span> <span class="o">=</span> <span class="nc">MixturePathGeneralizedKL</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="n">path</span><span class="p">)</span>  <span class="c1"># Generalized KL Bregman divergence  
</span>
<span class="k">for</span> <span class="n">x_0</span><span class="p">,</span> <span class="n">x_1</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>  <span class="c1"># Samples from π0,1 of shape [batch_size, *data_dim]  
</span>    <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="mf">1e-3</span><span class="p">)</span>  <span class="c1"># Randomize time t ∼ U [0, 1 − 10−3]  
</span>    <span class="n">sample</span><span class="p">:</span> <span class="n">DiscretePathSample</span> <span class="o">=</span> <span class="n">path</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="n">t</span><span class="p">,</span> <span class="n">x_0</span><span class="o">=</span><span class="n">x_0</span><span class="p">,</span> <span class="n">x_1</span><span class="o">=</span><span class="n">x_1</span><span class="p">)</span>  <span class="c1"># Sample the conditional path  
</span>    <span class="n">model_output</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">sample</span><span class="p">.</span><span class="n">x_t</span><span class="p">,</span> <span class="n">sample</span><span class="p">.</span><span class="n">t</span><span class="p">)</span>  

    <span class="n">loss</span> <span class="o">=</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">model_output</span><span class="p">,</span> <span class="n">x_1</span><span class="o">=</span><span class="n">sample</span><span class="p">.</span><span class="n">x_1</span><span class="p">,</span> <span class="n">x_t</span><span class="o">=</span><span class="n">sample</span><span class="p">.</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="n">sample</span><span class="p">.</span><span class="n">t</span><span class="p">)</span>  <span class="c1"># CDFM loss  
</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>  
    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>  
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>  

<span class="k">class</span> <span class="nc">ProbabilityDenoiser</span><span class="p">(</span><span class="n">ModelWrapper</span><span class="p">):</span>  
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">**</span><span class="n">extras</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>  
        <span class="n">logits</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="o">**</span><span class="n">extras</span><span class="p">)</span>  
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">.</span><span class="nf">float</span><span class="p">(),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  

<span class="c1"># Sample X1  
</span><span class="n">probability_denoiser</span> <span class="o">=</span> <span class="nc">ProbabilityDenoiser</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>  
<span class="n">x_0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">*</span><span class="n">data_dim</span><span class="p">])</span>  <span class="c1"># Specify the initial condition  
</span><span class="n">solver</span> <span class="o">=</span> <span class="nc">MixtureDiscreteEulerSolver</span><span class="p">(</span>  
    <span class="n">model</span><span class="o">=</span><span class="n">probability_denoiser</span><span class="p">,</span>  
    <span class="n">path</span><span class="o">=</span><span class="n">path</span><span class="p">,</span>  
    <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">vocabulary_size</span>  
<span class="p">)</span>  

<span class="n">step_size</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">100</span>  
<span class="n">x_1</span> <span class="o">=</span> <span class="n">solver</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">x_init</span><span class="o">=</span><span class="n">x_0</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="n">step_size</span><span class="p">,</span> <span class="n">time_grid</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="mf">1e-3</span><span class="p">]))</span>
</code></pre></div></div> <p>一个独立的DFM的代码案例如下：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>  
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>  
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">Tensor</span>  
<span class="kn">from</span> <span class="n">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_moons</span>  

<span class="k">class</span> <span class="nc">DiscreteFlow</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>  
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">h</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span> <span class="n">v</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">):</span>  
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>  
        <span class="n">self</span><span class="p">.</span><span class="n">v</span> <span class="o">=</span> <span class="n">v</span>  
        <span class="n">self</span><span class="p">.</span><span class="n">embed</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span>  
        <span class="n">self</span><span class="p">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>  
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">dim</span> <span class="o">*</span> <span class="n">h</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ELU</span><span class="p">(),</span>  
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ELU</span><span class="p">(),</span>  
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ELU</span><span class="p">(),</span>  
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">dim</span> <span class="o">*</span> <span class="n">v</span><span class="p">)</span>  
        <span class="p">)</span>  

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x_t</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>  
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">net</span><span class="p">(</span>  
            <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">(</span>  
                <span class="p">(</span><span class="n">t</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">],</span> <span class="n">self</span><span class="p">.</span><span class="nf">embed</span><span class="p">(</span><span class="n">x_t</span><span class="p">).</span><span class="nf">flatten</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span> <span class="o">-</span><span class="mi">1</span>  
            <span class="p">)</span>  
        <span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">x_t</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">])</span>  

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">256</span>  
<span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">128</span>  

<span class="n">model</span> <span class="o">=</span> <span class="nc">DiscreteFlow</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">)</span>  
<span class="n">optim</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>  

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>  
    <span class="n">x_1</span> <span class="o">=</span> <span class="nc">Tensor</span><span class="p">(</span><span class="nf">make_moons</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>  
    <span class="n">x_1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="n">x_1</span> <span class="o">*</span> <span class="mi">35</span> <span class="o">+</span> <span class="mi">50</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="n">vocab_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)).</span><span class="nf">long</span><span class="p">()</span>  
    <span class="n">x_0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>  

    <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>  
    <span class="n">x_t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">t</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">],</span> <span class="n">x_1</span><span class="p">,</span> <span class="n">x_0</span><span class="p">)</span>  

    <span class="n">logits</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>  
    <span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">.</span><span class="nf">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">x_1</span><span class="p">.</span><span class="nf">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)).</span><span class="nf">mean</span><span class="p">()</span>  
    <span class="n">optim</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>  
    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>  
    <span class="n">optim</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>  

<span class="n">x_t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>  
<span class="n">t</span> <span class="o">=</span> <span class="mf">0.0</span>  
<span class="n">results</span> <span class="o">=</span> <span class="p">[(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">)]</span>  
<span class="k">while</span> <span class="n">t</span> <span class="o">&lt;</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="mf">1e-3</span><span class="p">:</span>  
    <span class="n">p1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="nf">model</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span> <span class="o">*</span> <span class="n">t</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  
    <span class="n">h</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">t</span><span class="p">)</span>  
    <span class="n">one_hot_x_t</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">one_hot</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span>  
    <span class="n">u</span> <span class="o">=</span> <span class="p">(</span><span class="n">p1</span> <span class="o">-</span> <span class="n">one_hot_x_t</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">t</span><span class="p">)</span>  
    <span class="n">x_t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">distributions</span><span class="p">.</span><span class="nc">Categorical</span><span class="p">(</span><span class="n">probs</span><span class="o">=</span><span class="n">one_hot_x_t</span> <span class="o">+</span> <span class="n">h</span> <span class="o">*</span> <span class="n">u</span><span class="p">).</span><span class="nf">sample</span><span class="p">()</span>  
    <span class="n">t</span> <span class="o">+=</span> <span class="n">h</span>  
    <span class="n">results</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">))</span>  

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">results</span><span class="p">),</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  
<span class="nf">for </span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">),</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">axes</span><span class="p">):</span>  
    <span class="n">ax</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x_t</span><span class="p">.</span><span class="nf">detach</span><span class="p">()[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x_t</span><span class="p">.</span><span class="nf">detach</span><span class="p">()[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>  
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">t=</span><span class="si">{</span><span class="n">t</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>  
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>  
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <p>DFM生成效果如下：</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post/DFM33-480.webp 480w,/assets/img/post/DFM33-800.webp 800w,/assets/img/post/DFM33-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post/DFM33.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> DFM生成效果 </div> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p><a href="https://zhuanlan.zhihu.com/p/18450992825">https://zhuanlan.zhihu.com/p/18450992825</a> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:1:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:1:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a> <a href="#fnref:1:3" class="reversefootnote" role="doc-backlink">&#8617;<sup>4</sup></a> <a href="#fnref:1:4" class="reversefootnote" role="doc-backlink">&#8617;<sup>5</sup></a> <a href="#fnref:1:5" class="reversefootnote" role="doc-backlink">&#8617;<sup>6</sup></a> <a href="#fnref:1:6" class="reversefootnote" role="doc-backlink">&#8617;<sup>7</sup></a></p> </li> <li id="fn:2"> <p>Lipman Y, Chen R T Q, Ben-Hamu H, et al. Flow matching for generative modeling[J]. arXiv preprint arXiv:2210.02747, 2022. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:2:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p> </li> <li id="fn:3"> <p>Liu X, Gong C, Liu Q. Flow straight and fast: Learning to generate and transfer data with rectified flow[J]. arXiv preprint arXiv:2209.03003, 2022. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:5"> <p><a href="https://zhuanlan.zhihu.com/p/16026053810">https://zhuanlan.zhihu.com/p/16026053810</a> <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:5:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:5:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a></p> </li> <li id="fn:6"> <p>Gat I, Remez T, Shaul N, et al. Discrete flow matching[J]. arXiv preprint arXiv:2407.15595, 2024. <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:6:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="Flow_Matching"/><category term="Discrete_Flow_Matching"/><category term="formatting"/><category term="math"/><summary type="html"><![CDATA[接着上次说的DFM的分解技巧，继续学习DFM的最后一部分——混合路径（Mixture paths）。 混合路径（Mixture paths） 我们在此前的文章里面已经讲解了分解技巧1，得到了分解的（边缘/条件）概率路径、分解的（边缘/条件）速率，以及知道了他们之间的关系（1中的命题2和定理16）。现在我们要着手设计出一个具体的DFM了，类似FM2一样，我们可以从分解条件概率路径着手，然后得到目标分解条件速率，使用神经网络预测分解条件速率，最后使用分解CDFM损失（1中公式14）来训练模型。 我们以$Z=(X_0,X_1) \sim \pi_{0,1}(X_0,X_1)$为条件（$X_0$和$X_1$为任意配对），那么分解的条件概率路径可以使用如下形式： \[p_{t\mid 0,1}^i (x^i\mid x_0,x_1)=\kappa_t\delta(x^i,x_1^i) + (1-\kappa_t)\delta(x^i,x_0^i) \tag{1}\] 这个做法类似Rectified Flow（3中公式1），只是使用了更general的$\kappa_t$作为系数，$\kappa : [0,1] \to [0,1]$就类似扩散模型里面的噪声调度器，$\kappa_t \in C^1([0,1])$。根据1中公式6，我们能得到非分解的条件概率路径： \[p_{t\mid 0,1} (x\mid x_0,x_1)=\prod_i p_{t\mid 0,1}^i (x^i\mid x_0,x_1) \tag{2}\] 关于公式1的详细解释： 分解概率路径上的随机变量$X_t^i \sim p_{t\mid 0,1}^i(\cdot\mid x_0,x_1) $满足： \[X_t^i = \begin{cases} x_1^i \quad 概率为\kappa_t\\ x_0^i \quad 概率为(1-\kappa_t) \end{cases} \tag{3}\] 也就是对$t$时刻的第$i$个token $X_t^i$，它要么取源状态的值$x_0^i$，要么取目标状态的值$x_1^i$，取这两个值的概率与时间$t$相关。 从公式1和2给出的条件概率路径可以算出边缘概率路径，即： \[p_t(x)=\sum_{x_0,x_1}p_{t\mid 0,1}(x\mid x_0,x_1) p_{0,1}(x_0,x_1) \tag{4}\] 需要指出的是，$p_t(x)$需要满足边缘条件，即$p_0(x) = \delta(x,x_0)$，$p_1(x) = \delta(x,x_1)$。为实现这个条件，需要让$\kappa_0 = 0$，$\kappa_1 = 1$。 接下来，我们需要知道公式1所示分解条件概率路径对应的分解条件速率$u_t^i(y^i,x^i\mid x_0,x_1)$。由Kolmogorov方程可知： \[\cfrac{\mathrm{d}}{\mathrm{d}t}p_{t\mid 0,1}^i(y^i\mid x_0,x_1) = \sum_{x^i}u_t^i(y^i,x^i\mid x_0,x_1)p_{t\mid 0,1}^i(x^i\mid x_0,x_1) \tag{5}\] 结合公式1给出的具体分解条件概率路径，可以算出： \[\begin{aligned} \cfrac{\mathrm{d}}{\mathrm{d}t}p_{t\mid 0,1}^i(y^i\mid x_0,x_1)&amp; \overset{公式1}{=} \dot \kappa_t\left[\delta(y^i,x_1^i) - \delta(y^i,x_0^i)\right] \\ &amp;\overset{用公式1消去\delta(y^i,x_0^i)}{=} \dot \kappa_t\left[\delta(y^i,x_1^i) - \cfrac{p^i_{t\mid 0,1}(y^i\mid x_0,x_1)-\kappa_t\delta(y^i,x_1^i)}{1-\kappa_t}\right]\\ &amp;{=} \cfrac{\dot \kappa_t}{1 - \kappa_t}\left[({1-\kappa_t})\delta(y^i,x_1^i) - {p^i_{t\mid 0,1}(y^i\mid x_0,x_1)+\kappa_t\delta(y^i,x_1^i)}\right]\\ &amp;{=} \cfrac{\dot \kappa_t}{1 - \kappa_t} \left[ \delta(y^i,x_1^i) - p^i_{t\mid 0,1}(y^i\mid x_0,x_1) \right]\\ &amp;\overset{(*)}{=}\sum_{x^i} \cfrac{\dot \kappa_t}{1 - \kappa_t} \left[ \delta(y^i,x_1^i) - \delta(y^i,x^i) \right] p^i_{t\mid 0,1}(x^i\mid x_0,x_1)\\ \end{aligned}\tag{6}\] 关于（*）步的解释如下： 这里其实就是要证明 \[\delta(y^i,x_1^i) - p^i_{t\mid 0,1}(y^i\mid x_0,x_1) = \sum_{x^i} \left[ \delta(y^i,x_1^i) - \delta(y^i,x^i) \right] p^i_{t\mid 0,1}(x^i\mid x_0,x_1) \tag{7}\] 我们从右边开始变换： \[\begin{aligned} \sum_{x^i} \left[ \delta(y^i,x_1^i) - \delta(y^i,x^i) \right] p^i_{t\mid 0,1}(x^i\mid x_0,x_1) &amp;= \sum_{x^i} \left[ \delta(y^i,x_1^i)p^i_{t\mid 0,1}(x^i\mid x_0,x_1) - \delta(y^i,x^i)p^i_{t\mid 0,1}(x^i\mid x_0,x_1) \right]\\ &amp;= \sum_{x^i} \delta(y^i,x_1^i)p^i_{t\mid 0,1}(x^i\mid x_0,x_1) - \sum_{x^i} \delta(y^i,x^i)p^i_{t\mid 0,1}(x^i\mid x_0,x_1) \\ &amp;\overset{(1)}{=} \sum_{x^i} \delta(y^i,x_1^i)p^i_{t\mid 0,1}(x^i\mid x_0,x_1) - p^i_{t\mid 0,1}(y^i\mid x_0,x_1) \\ &amp;\overset{(2)}{=} \delta(y^i,x_1^i) - p^i_{t\mid 0,1}(y^i\mid x_0,x_1) \\ \end{aligned} \tag{8}\] 第（1）步是delta函数的性质，$\int_x\delta(y,x)f(x)\mathrm{d}x = f(y)$，第（2）步是因为$\delta(y^i,x_1^i)$与$x^i$无关，$\sum_{x^i} p^i_{t\mid 0,1}(x^i\mid x_0,x_1) = 1 $。 对比公式6的最后结果和公式5，就知道： \[u_t^i(y^i,x^i\mid x_0,x_1) = \cfrac{\dot \kappa_t}{1 - \kappa_t} \left[ \delta(y^i,x_1^i) - \delta(y^i,x^i) \right] \tag{9}\] 这样我们就知道了公式1所示的分解条件概率路径对应的分解条件速率$u_t^i(y^i,x^i\mid x_0,x_1)$的具体形式了。 Code 9是混合路径的实现代码。 混合路径的实现代码 速率后验参数化 类比我们在FM2和扩散模型里面也会讲的多种预测方式，例如velocity-prediction、$x_0$-prediction、$x_1$-prediction，我们在DFM里面也可以使用类似的预测方式。 最简单的一种：使用神经网络预测公式9给出的分解条件速率$u_t^i(y^i,x^i\mid x_0,x_1)$，这就是velocity-prediction。 接下来介绍DFM里面的$x_1$-prediction，这个会稍微复杂一点。 根据我们在1中证明出来的公式12，可以得到在$u_t^i(y^i,x^i\mid x_0,x_1)$取公式9的形式时，分解边缘速率为： \[\begin{aligned} u_t^i(y^i,x) &amp;=\sum_{x_0,x_1} u_t^i(y^i,x^i\mid x_0,x_1)p_{0,1\mid t}(x_0,x_1\mid x)\\ &amp;\overset{公式(9)}{=} \sum_{x_0,x_1} \cfrac{\dot \kappa_t}{1 - \kappa_t} \left[ \delta(y^i,x_1^i) - \delta(y^i,x^i) \right] p_{0,1\mid t}(x_0,x_1\mid x)\\ &amp;\overset{将x_1^i分出来}{=} \sum_{x_1^i}\sum_{x_0,x_1^{\bar i}} \cfrac{\dot \kappa_t}{1 - \kappa_t} \left[ \delta(y^i,x_1^i) - \delta(y^i,x^i) \right] p_{0,1\mid t}(x_0,x_1\mid x)\\ &amp;\overset{分配律}{=} \sum_{x_1^i} \cfrac{\dot \kappa_t}{1 - \kappa_t} \left[ \delta(y^i,x_1^i) - \delta(y^i,x^i) \right]\sum_{x_0,x_1^{\bar i}} p_{0,1\mid t}(x_0,x_1\mid x)\\ \end{aligned} \tag{10}\] 单独把第二个求和式拿出来，记为： \[p_{1\mid t}^i(x_1^i\mid x) = \sum_{x_0,x_1^{\bar i}} p_{0,1\mid t}(x_0,x_1\mid x) \overset{(*)}{=}\mathbb{E}\left[ \delta(x_1^i,X_1^i) \mid X_t=x \right] \tag{11}\] 关于公式11中（*）步骤的详细说明： \[\begin{aligned} p_{1\mid t}^i(x_1^i\mid x) &amp;= \sum_{x_0,x_1^{\bar i}} p_{0,1\mid t}(x_0,x_1\mid x)\\ &amp;\overset{拆开x_1}{=} \sum_{x_0,x_1^{\bar i}} p_{0,1\mid t}(x_0,x_1^{\bar i},x_1^i\mid x)\\ &amp;\overset{\delta函数的性质}{=} \sum_{x_0,x_1^{\bar i}}\sum_{X_1^i}\delta(x_1^i,X_1^i) p_{0,1\mid t}(x_0,x_1^{\bar i},X_1^i\mid x)\\ &amp;=\mathbb{E}\left[ \delta(x_1^i,X_1^i) \mid X_t=x \right] \end{aligned} \tag{12}\] 我们可以用神经网络去学习后验分布$p_{1\mid t}^{\theta,i}(x_1^i\mid x)$，$\theta$为神经网络的参数。这就是离散版的$x_1$-prediction。 这样理解：以文本为例，$x$是时间$t$时的“加噪”文本（但其实说“加噪“不准确，因为DFM的初始状态$x_0$是一个全mask的文本，$x$更类似一个处于中间状态的文本）。$x$作为神经网络的输入，神经网络预测$x_1^i$，$i\in [d]$表示这是$x_1$的第$i$个token，$d$为句长。神经网络一次预测所有token的单步变化概率，每个token共有$K$种变化情况（$K$为vocabulary的大小），神经网络的输出向量的维度就是$d\cdot K$。 混合路径的CDFM损失 我们基于$x_1$-prediction，即预测$p_{1\mid t}^{\theta,i}(x_1^i\mid x)$，继续讨论公式1所示混合路径情况下的CDFM损失。这里会有两种做法。 结合公式11，如果直接对比后验概率，则CDFM损失可以写为： \[L_{CDFM}(\theta) = \mathbb{E}_{t,X_0,X_1,X_t} D_{X_t}\left( \delta(\cdot,X_1^i),p_{1\mid t}^{\theta,i}(\cdot\mid X_t) \right) \tag{13}\] $\delta(\cdot,X_1^i)$，$p_{1\mid t}^{\theta,i}(\cdot\mid X_t)$都是概率质量函数，所以可以使用KL散度衡量二者之间的距离，即取Bregman散度为KL散度，得到： \[L_{CDFM}(\theta) = - \mathbb{E}_{t,X_0,X_1,X_t} \log p_{1\mid t}^{\theta,i}(X_1^i\mid X_t) + \text{const} \tag{14}\] 这是第一种做法。 关于公式14的详细说明： KL散度的公式是：$D(p,q)=\sum_{\alpha \in \mathcal{T}}p(\alpha)\log\cfrac{p(\alpha)}{q(\alpha)}$。在选择KL散度为Bregman散度后，公式13可以化为： \[\begin{aligned} L_{CDFM}(\theta) &amp;= \mathbb{E}_{t,X_0,X_1,X_t} \sum_{X_1^i} \delta(x_1^i,X_1^i) \log\cfrac{\delta(x_1^i,X_1^i)}{p_{1\mid t}^{\theta,i}( X_1^i\mid X_t)}\\ &amp;=\mathbb{E}_{t,X_0,X_1,X_t} \sum_{X_1^i}\left[ \delta(x_1^i,X_1^i) \log{\delta(x_1^i,X_1^i)} - \delta(x_1^i,X_1^i) \log{p_{1\mid t}^{\theta,i}( X_1^i\mid X_t)}\right]\\ &amp;= \text{const} - \mathbb{E}_{t,X_0,X_1,X_t} \sum_{X_1^i}\delta(x_1^i,X_1^i) \log{p_{1\mid t}^{\theta,i}( X_1^i\mid X_t)}\\ &amp;\overset{\delta函数的性质}{=} \text{const} - \mathbb{E}_{t,X_0,X_1,X_t} \log{p_{1\mid t}^{\theta,i}(x_1^i\mid X_t)} \end{aligned} \tag{15}\] 第二种做法是使用1中公式14所示的分解CDFM损失，如下： \[L_{CDFM}(\theta) = \mathbb{E}_{t,Z,X_t\sim p_{t\mid Z}} \sum_i D_{X_t}^i \left( u_t^i(\cdot,X_t \mid Z), u_t^{\theta,i}(\cdot, X_t) \right) \tag{16}\] 取$Z=(X_0,X_1)$。使用该公式需要知道$u_t^{\theta,i}$，因为神经网络预测了$p_{1\mid t}^{\theta,i}$，所以$u_t^{\theta,i}$是能算出来的。 公式16中的Bregman散度可以选择广义的KL损失（其输入不一定要是概率分布）。具体而言，对于向量$u,v\in \mathbb R_{\ge 0}^{m}$，广义KL损失是： \[D(u,v) = \sum_j u^j \log \cfrac{u^j}{v^j} - \sum_j u_j + \sum_jv_j \tag{17}\] 这种选择下对应的Bregman散度为： \[D\left(u_t^i(\cdot,x^i\mid x_0,x_1),u^{i,\theta}_t(\cdot,x)\right)= \cfrac{\dot \kappa_t}{1-\kappa_t}\left[ \left(\delta(x_1^i,x^i) - 1\right)\log p_{1\mid t}^{i,\theta}(x_1^i\mid x) + \delta(x_1^i,x^i)- p_{1\mid t}^{i,\theta}(x^i\mid x) \right] \tag{18}\] 混合轨迹采样 我们在1中的公式5已经给出per coordinate的采样方式，结合前面讨论的混合路径，采样公式可以写为： \[\mathbb{P}(X^i_{t+h}=y^i\mid X_t=x) =\delta(y^i,x^i) + h u_t^i(y^i,x) +o(h) \tag{19}\] 在混合路径的特定情况下为： \[\begin{aligned} \mathbb{P}(X^i_{t+h}=y^i\mid X_t=x) &amp;\overset{公式10}{=} \delta(y^i,x^i) + o(h) + h \sum_{x_1^i} \cfrac{\dot \kappa_t}{1 - \kappa_t} \left[ \delta(y^i,x_1^i) - \delta(y^i,x^i) \right]\sum_{x_0,x_1^{\bar i}} p_{0,1\mid t}(x_0,x_1\mid x)\\ &amp;\overset{公式11}{=} \delta(y^i,x^i) + o(h) + h \sum_{x_1^i} \cfrac{\dot \kappa_t}{1 - \kappa_t} \left[ \delta(y^i,x_1^i) - \delta(y^i,x^i) \right] p_{1\mid t}^i(x_1^i\mid x)\\ &amp;\overset{\sum_{x_1^i}p_{1\mid t}^i(x_1^i\mid x)=1 }{=} \sum_{x_1^i}\left[ \delta(y^i,x^i) + h \cfrac{\dot \kappa_t}{1 - \kappa_t} \left[ \delta(y^i,x_1^i) - \delta(y^i,x^i) \right] + o(h) \right] p_{1\mid t}^i(x_1^i\mid x)\\ \end{aligned} \tag{20}\] 在已知$X_t = x$的情况下，使用公式20进行采样的具体步骤如下： 从$p_{1\mid t}^i(X_1^i\mid x)$采样出$X_1^i$，$X_1^i\sim p_{1\mid t}^i(X_1^i\mid x)$。$p_{1\mid t}^i(X_1^i\mid x)$是由模型预测得到。 基于$X_t^i$更新出$X_{t+h}^i$。这一步需要使用4中公式9所示的Euler step，并将其中的速率设置为$\cfrac{\dot \kappa_t}{1 -\kappa_t}\left[\delta(y^i,X_1^i) - \delta(y^i,x^i)\right]$。这一步决定了$X_{t+h}^i=X_t^i$还是$X_{t+h}^i=X_1^i$。 单边混合路径和保概率速率 我们在第一次讲CTMC的那篇文章4里面的最后一部分讲了一下保概率速率，到这里终于要用到了。 简单回顾一下在CTMC模型里面讲到的保概率速率，它简单的说就是在已有的速率$u_t(y,x)$中加入另一个速率$v_t(y,x)$，只要$v_t(y,x)$是divergence-free的，那么$u_t(y,x)$生成概率路径不会改变。所谓divergence-free，指$\sum_x v_t(y,x)p_t(x) = 0$。 对DFM里面的分解条件概率路径$p_{t\mid Z}^i(x^i\mid z)$，其对应的保概率条件速率$v_t^i(y^i,x^i\mid z)$满足： \[\sum_{x^i} v_t^i(y^i,x^i\mid z)p_{t\mid Z}^i(x^i\mid z) = 0 \tag{21}\] 一般来说这个保概率条件速率$v_t^i(y^i,x^i\mid z)$不是特别好找到。但是如果我们能有如下两个假设： iid的目标分布： $p(x) = \prod_i p(x^i)$。 源分布和目标分布的数据独立配对：$\pi_{0,1}(x_0,x_1) = p(x_0)q(x_1)$。 那么保概率条件速率的解析式还是比较好找到的。接下来我们来构造这个保概率条件速率。 根据前面的两条假设，能写出混合路径的形式如下： \[p_t(x) = \sum_{x_1} p_{t\mid 1}(x\mid x_1)q(x_1) \text{ where } p_{t\mid 1} (x) = \prod_i p_{t\mid 1}^i(x^i \mid x_1)\\ p_{t\mid 1}^i(x^i \mid x_1) = \kappa_t \delta(x^i,x_1^i) + (1 - \kappa_t)p(x^i) \tag{22}\] 这个写法和我们前面的混合概率路径的写法的差异就是只用$x_1$作为条件，但其实它和用$x_0,x_1$都作为条件应该是等价的。 类比公式6 \[\begin{aligned} \cfrac{\mathrm{d}}{\mathrm{d}t}p_{t\mid 1}^i(y^i\mid x_1) &amp; \overset{公式22}{=} \dot \kappa_t\left[\delta(y^i,x_1^i) - p(x^i)\right] \\ &amp;\overset{用公式22消去p(x^i)}{=} \dot \kappa_t\left[\delta(y^i,x_1^i) - \cfrac{p^i_{t\mid 1}(y^i\mid x_1)-\kappa_t\delta(y^i,x_1^i)}{1-\kappa_t}\right]\\ &amp;{=} \cfrac{\dot \kappa_t}{1 - \kappa_t}\left[({1-\kappa_t})\delta(y^i,x_1^i) - {p^i_{t\mid 1}(y^i\mid x_1)+\kappa_t\delta(y^i,x_1^i)}\right]\\ &amp;{=} \cfrac{\dot \kappa_t}{1 - \kappa_t} \left[ \delta(y^i,x_1^i) - p^i_{t\mid 1}(y^i\mid x_1) \right]\\ &amp;{=}\sum_{x^i} \cfrac{\dot \kappa_t}{1 - \kappa_t} \left[ \delta(y^i,x_1^i) - \delta(y^i,x^i) \right] p^i_{t\mid 1}(x^i\mid x_1)\\ \end{aligned}\tag{23}\] 对比Kolmogorov方程，知道： \[u_t^i(y^i,x^i\mid x_1)=\cfrac{\dot \kappa_t}{1 - \kappa_t} \left[ \delta(y^i,x_1^i) - \delta(y^i,x^i) \right] \tag{24}\] 可见$u_t^i(y^i,x^i\mid x_1)$生成$p^i_{t\mid 1}(x^i\mid x_1)$。如果要找到对$p^i_{t\mid 1}(x^i\mid x_1)$而言divergence-free的条件速率，一个简单的技巧是找到一个和$u_t^i(y^i,x^i\mid x_1)$方向相反的条件速率5，这样在总时间范围内折两个速率差的散度和就是0，所以这两速率的差就是我们要找的divergence-free的条件速率，也就是保概率条件速率。记这个反向的条件速率为$\tilde u_t^i(y^i,x^i\mid x_1)$，它的形式应该是： \[\tilde u_t^i(y^i,x^i\mid x_1) = \cfrac{\dot \kappa_t}{\kappa_t} \left[ \delta(y^i,x^i) - p(x^i) \right] \tag{25}\] 证明在5的E.5和E.6中。 所以divergence-free的速率是： \[v_t^i(y^i,x^i \mid x_1) =u_t^i(y^i,x^i\mid x_1) -\tilde u_t^i(y^i,x^i\mid x_1)\tag{26}\] 因为$v_t^i(y^i,x^i \mid x_1)$是divergence-free的，所以把它以任意比例加到原来的速率里面都不会影响生成的概率路径。所以公式10所示分解边缘速率如果改写成下面的形式： \[\begin{aligned} u_t^i(y^i,x) &amp;=\sum_{x_1} \left[ u_t^i(y^i,x^i\mid x_1) + c_t v_t^i(y^i,x^i\mid x_1) \right]p_{1\mid t}(x_1 \mid x)\\ &amp;=\sum_{x_1^i} \left[ u_t^i(y^i,x^i\mid x_1^i) + c_t v_t^i(y^i,x^i\mid x_1^i) \right]p^i_{1\mid t}(x^i_1 \mid x)\\ \end{aligned} \tag{27}\\\] 不影响所生成的边缘概率路径。第二个等式是因为$u_t^i(y^i,x^i\mid x_1) =u_t^i(y^i,x^i\mid x_1^i)$，$v_t^i(y^i,x^i\mid x_1) =v_t^i(y^i,x^i\mid x_1^i)$。 那么，用公式27所示的速率进行采样的步骤是： 从$p_{1\mid t}^i(X_1^i\mid x)$采样出$X_1^i$，$X_1^i\sim p_{1\mid t}^i(X_1^i\mid x)$。$p_{1\mid t}^i(X_1^i\mid x)$是由模型预测得到。 基于$X_t^i$更新出$X_{t+h}^i$。这一步需要使用4中公式9所示的Euler step，并将其中的速率设置为 \[u_t^i(y^i,x^i\mid x_1) = \cfrac{\dot \kappa_t}{1-\kappa_t}\left[\delta(y^i,X_1^i) - \delta(y^i,x^i)\right] +c_t\left[ \cfrac{\dot \kappa_t}{1-\kappa_t}\left[\delta(y^i,x_1^i) - \delta(y^i,x^i)\right] - \cfrac{\dot \kappa_t}{\kappa_t} \left[\delta(y^i,x^i) - p(x^i)\right] \right] \tag{28}\] 其中$c_t&gt;0$是与时间相关的常数。 在https://github.com/facebookresearch/flow_matching库里面实现的DFM的代码如下： import torch from flow_matching.path import MixtureDiscreteProbPath, DiscretePathSample from flow_matching.path.scheduler import PolynomialConvexScheduler from flow_matching.loss import MixturePathGeneralizedKL from flow_matching.solver import MixtureDiscreteEulerSolver from flow_matching.utils import ModelWrapper # Define a trainable velocity model model=... optimizer=torch.optim.Adam(model.parameters()) scheduler=PolynomialConvexScheduler(n=1.0) path=MixtureDiscreteProbPath(scheduler=scheduler) loss_fn=MixturePathGeneralizedKL(path=path) # Generalized KL Bregman divergence for x_0, x_1 in dataloader: # Samples from π0,1 of shape [batch_size, *data_dim] t=torch.rand(batch_size) * (1.0 - 1e-3) # Randomize time t ∼ U [0, 1 − 10−3] sample: DiscretePathSample=path.sample(t=t, x_0=x_0, x_1=x_1) # Sample the conditional path model_output=model(sample.x_t, sample.t) loss=loss_fn(logits=model_output, x_1=sample.x_1, x_t=sample.x_t, t=sample.t) # CDFM loss optimizer.zero_grad() loss.backward() optimizer.step() class ProbabilityDenoiser(ModelWrapper): def forward(self, x: torch.Tensor, t: torch.Tensor, **extras) -&gt; torch.Tensor: logits=self.model(x, t, **extras) return torch.nn.functional.softmax(logits.float(), dim=-1) # Sample X1 probability_denoiser=ProbabilityDenoiser(model=model) x_0=torch.randint(size=[batch_size, *data_dim]) # Specify the initial condition solver=MixtureDiscreteEulerSolver( model=probability_denoiser, path=path, vocabulary_size=vocabulary_size ) step_size=1 / 100 x_1=solver.sample(x_init=x_0, step_size=step_size, time_grid=torch.tensor([0.0, 1.0 - 1e-3])) 一个独立的DFM的代码案例如下： import torch import matplotlib.pyplot as plt from torch import nn, Tensor from sklearn.datasets import make_moons class DiscreteFlow(nn.Module): def __init__(self, dim: int=2, h: int=128, v: int=128): super().__init__() self.v = v self.embed = nn.Embedding(v, h) self.net = nn.Sequential( nn.Linear(dim * h + 1, h), nn.ELU(), nn.Linear(h, h), nn.ELU(), nn.Linear(h, h), nn.ELU(), nn.Linear(h, dim * v) ) def forward(self, x_t: Tensor, t: Tensor) -&gt; Tensor: return self.net( torch.cat( (t[:, None], self.embed(x_t).flatten(1, 2)), -1 ) ).reshape(list(x_t.shape) + [self.v]) batch_size=256 vocab_size=128 model=DiscreteFlow(v=vocab_size) optim=torch.optim.Adam(model.parameters(), lr=0.001) for _ in range(10000): x_1=Tensor(make_moons(batch_size, noise=0.05)[0]) x_1=torch.round(torch.clip(x_1 * 35 + 50, min=0.0, max=vocab_size - 1)).long() x_0=torch.randint(low=0, high=vocab_size, size=(batch_size, 2)) t=torch.rand(batch_size) x_t=torch.where(torch.rand(batch_size, 2) &lt; t[:, None], x_1, x_0) logits=model(x_t, t) loss=nn.functional.cross_entropy(logits.flatten(0, 1), x_1.flatten(0, 1)).mean() optim.zero_grad() loss.backward() optim.step() x_t=torch.randint(low=0, high=vocab_size, size=(200, 2)) t=0.0 results=[(x_t, t)] while t &lt; 1.0 - 1e-3: p1=torch.softmax(model(x_t, torch.ones(200) * t), dim=-1) h=min(0.1, 1.0 - t) one_hot_x_t=nn.functional.one_hot(x_t, vocab_size).float() u=(p1 - one_hot_x_t) / (1.0 - t) x_t=torch.distributions.Categorical(probs=one_hot_x_t + h * u).sample() t += h results.append((x_t, t)) fig, axes=plt.subplots(1, len(results), figsize=(15, 2), sharex=True, sharey=True) for (x_t, t), ax in zip(results, axes): ax.scatter(x_t.detach()[:, 0], x_t.detach()[:, 1], s=10) ax.set_title(f't={t:.1f}') plt.tight_layout() plt.show() DFM生成效果如下： DFM生成效果 https://zhuanlan.zhihu.com/p/18450992825 &#8617; &#8617;2 &#8617;3 &#8617;4 &#8617;5 &#8617;6 &#8617;7 Lipman Y, Chen R T Q, Ben-Hamu H, et al. Flow matching for generative modeling[J]. arXiv preprint arXiv:2210.02747, 2022. &#8617; &#8617;2 Liu X, Gong C, Liu Q. Flow straight and fast: Learning to generate and transfer data with rectified flow[J]. arXiv preprint arXiv:2209.03003, 2022. &#8617; https://zhuanlan.zhihu.com/p/16026053810 &#8617; &#8617;2 &#8617;3 Gat I, Remez T, Shaul N, et al. Discrete flow matching[J]. arXiv preprint arXiv:2407.15595, 2024. &#8617; &#8617;2]]></summary></entry><entry><title type="html">离散型流匹配模型（2）</title><link href="https://ly403.github.io/blog/2025/%E7%A6%BB%E6%95%A3%E6%B5%81%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-2/" rel="alternate" type="text/html" title="离散型流匹配模型（2）"/><published>2025-01-19T00:00:00+00:00</published><updated>2025-01-19T00:00:00+00:00</updated><id>https://ly403.github.io/blog/2025/%E7%A6%BB%E6%95%A3%E6%B5%81%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B%EF%BC%882%EF%BC%89</id><content type="html" xml:base="https://ly403.github.io/blog/2025/%E7%A6%BB%E6%95%A3%E6%B5%81%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-2/"><![CDATA[<p>接着上一次学习的DFM的上半部分，这次学习DFM的下半部分。主要还是基于Flow Matching Guide and Code<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>。</p> <h2 id="分解路径速度和损失">分解路径、速度和损失</h2> <p>在实际实现DFM的时候，我们会想要像FM<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>一样用神经网络预测上一节说的条件速度场，即$u^{\theta}_t(y,x) = \mathbb{E}[u_t(y,X_t\mid Z )\mid X_t =x )]$。但是，如果这样做，那么神经网络需要处理非常多种情况。具体而言，因为$y\in \mathcal{S}=\mathcal{T}^d$，其中$\mathcal{T} = {1,\cdots, K}$是vocabulary，$d$是句子长度，所以神经网络输出的大小为$K^d$（对第一个词要考虑$K$个状态，对第二个、第三个直到第$d$个词都要考虑$K$个状态，所以是$K^d$）。这么大的输出大小是不可实现的。为解决这个问题，可以使用<strong>分解（factorized）<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>方法</strong>。</p> <p>下面要逐个使用这种方法来分解（边缘/条件）路径、（边缘/条件）速度和条件损失。</p> <h3 id="分解边缘速度">分解边缘速度</h3> <p>分解方法的核心思想在于：<strong>只考虑最多一个token的改变</strong>。因为多个token的改变可以视为逐个token的单步变化的叠加。探讨一个token的改变就像探讨基变换一样。</p> <p>分解边缘速度的具体形式是：</p> \[u_t(y,x) = \sum_i \delta(y^{\bar{i}},x^{\bar{i}})u_t^i(y^i,x)\tag{1}\] <p>$\bar i = {1,\cdots, i-1,i+1,\cdots,d}$，就是把$i$去掉，相当于只考虑第$i$个token的变化。分解边缘速度如下图所示：</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post/DFM2-480.webp 480w,/assets/img/post/DFM2-800.webp 800w,/assets/img/post/DFM2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post/DFM2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 分解边缘速度图示 </div> <p>现在我们只需要使用模型预测$u_t^{i}(y^i,x)$了。其中，$i\in [d] = {1,\cdots,d}$，$y^i \in \mathcal{T}$是第$i$个token状态改变后所在的状态，$x\in \mathcal{S}$为状态转移之前的全部token的状态，$u_t^i(y^i,x) \in \mathbb R$是一个标量速率。所以，<strong>模型要输出的结果的维度是$d\cdot K$（总共$d$个token，每个token有$K$种可以改变到的状态，对应$d\cdot K$种速率），相比之前的$K^d$大大减小</strong>。</p> <p>分解的边缘速率还是要满足速率条件，不过是要按照每个token的层级进行描述，如下：</p> \[u_t^i(y^i,x)\ge 0 \text{ for all } y^i \neq x^i \text{, and } \sum_{y^i \in \mathcal{T}}{u^i_t(y^i,x)} = 0 \quad \text{for all }x \in \mathcal{S} \tag{2}\] <p>$\forall i\in [d]$，公式（2）都要满足。</p> <p>既然我们有了分解边缘概率，那么采样过程也可以coordinate-wise（逐维）地进行，代入公式（1）到CTMC模型的转移核之中，即：</p> \[\begin{aligned} \mathbb{P}(X_{t+h}=y\mid X_t = x)&amp;=\delta(y,x)+hu_t(y,x)+o(h)\\ &amp;\overset{代入公式(1)}{=} \delta(y,x)+h\sum_i \delta(y^{\bar{i}},x^{\bar{i}})u_t^i(y^i,x)+o(h)\\ &amp;\overset{公式(4)}{=} \prod_{i}\left[ \delta(y^i,x^i) + h u_t^i(y^i,x) +o(h) \right] \end{aligned} \tag{3}\] <blockquote> <p>公式（3）第二个等号基于如下等式：</p> \[\prod_i\left[ a^i + h b^i \right] = \prod_i a^i + h\sum_{i}\left(\prod_{j\neq i}a^j\right) b^i + o(h) \tag{4}\] <p>取$a^i = \delta(y^i,x^i)$，$b^i = u_t^i(y^i,x)$，同时注意到$\prod_i \delta(y^i,x^i) = \delta(y,x)$，就得出了公式（3）中第二个等号表示的关系。</p> </blockquote> <p>所以我们还可以用分解边缘速率定义一个分解转移核用以采样：</p> \[\begin{aligned} \mathbb{P}(X^i_{t+h}=y^i\mid X_t = x) &amp;=\delta(y^i,x^i) + h u_t^i(y^i,x) +o(h) \\ \mathbb{P}(X_{t+h}=y\mid X_t = x) &amp;=\prod_i\mathbb{P}(X^i_{t+h}=y^i\mid X_t = x) \end{aligned}\tag{5}\] <p>采样方法还是可以用CTMC模型里面说过的Euler法，但是这里是per coordinate的采样，即每一维单独进行采样。</p> <p>其实FM的采样也可以写成类似的分解的形式，但一般不这么用。值得一提的是：<strong>FM的采样是确定性的，而DFM的采样具有随机性</strong>。</p> <h3 id="分解边缘概率路径">分解边缘概率路径</h3> <p>上面我们已经实现了分解边缘速度，现在考虑用一样的方法分解边缘概率路径$q_t(x)$：</p> \[q_t(x) = \prod_{i}q_t^i(x^i)\tag{6}\] <p>其中，$q_t^i(x^i)$就是分解后的边缘概率路径。</p> <p>接下来我们想说明：<strong>分解边缘速率生成分解边缘概率路径时，非分解的边缘速率也能生成非分解的边缘概率路径</strong>，也就是希望边缘速率和边缘概率路径的关系在分解和非分解形式下都保留。具体而言就是如下命题：</p> <p><strong>命题2</strong> 令$q_t(x)$由公式（6）所示的分解形式定义，当$u_t^i(y^i,x^i)\in C([0,1))$能够生成$q_t^i(x^i)$时，那么$q_t$由如下分解形式的边缘速率生成：</p> \[u_t(y,x)=\sum_i\delta(y^{\bar i},x^{\bar i})u_t^i(y^i,x^i) \tag{7}\] <blockquote> <p>命题2的证明如下：</p> <p>首先根据边缘概率的定义，我们可以得到$x^i$和$x^{\bar i}$的边缘概率可以通过如下方式计算（就是把无关的变量求和掉）：</p> \[q^i(x^i) := \sum_{x^{\bar i}}q(x) \quad q^{\bar i }(x^{\bar i }) :=\sum_{x^i} q(x) \tag{8}\] <p>推导如下：</p> \[\begin{aligned} \cfrac{\mathrm{d}}{\mathrm{d}t}q_t(y) &amp;\overset{代入公式(6)}{=} \cfrac{\mathrm{d}}{\mathrm{d}t}\prod_i q_t^i(y^i)\\ &amp;\overset{乘法求导法则}{=} \sum_i q^{\bar i}_t(y^{\bar i}) \cfrac{\mathrm{d}}{\mathrm{d}t}q_t^i(y^i)\\ &amp;\overset{\text{Kolmogorov}方程}{=} \sum_i q^{\bar i}_t(y^{\bar i}) \left[ \sum_{x^i} u_t^i(y^i,x^i)q_t^i(x^i) \right]\\ &amp;\overset{(*)}{=} \sum_i \left[ \sum_{x^{\bar i}}\delta(y^{\bar i}, x^{\bar i} ) q_t^{\bar i}(x^{\bar i}) \right] \left[ \sum_{x^i} u_t^i(y^i,x^i)q_t^i(x^i) \right]\\ &amp;\overset{分配律}{=} \sum_i \sum_{x^{\bar i}}\sum_{x^i} \left[ \delta(y^{\bar i}, x^{\bar i} ) q_t^{\bar i}(x^{\bar i}) u_t^i(y^i,x^i)q_t^i(x^i) \right]\\ &amp;\overset{合并x^i和x^{\bar i}}{=} \sum_i \sum_{x} \left[ \delta(y^{\bar i}, x^{\bar i} ) u_t^i(y^i,x^i)q_t(x) \right]\\ &amp;\overset{交换求和顺序}{=} \sum_{x}\left[\sum_i \left[ \delta(y^{\bar i}, x^{\bar i} ) u_t^i(y^i,x^i) \right]q_t(x)\right]\\ \end{aligned} \tag{9}\] <p>（*）式基于$q_t^{\bar i }(y^{\bar i}) = \sum_{x^{\bar i}}\delta(y^{\bar i}, x^{\bar i})q_t^{\bar i}(x^{\bar i})$。根据最后结果，对比Kolmogorov方程，就知道$u_t(y,x)=\sum_i\delta(y^{\bar i},x^{\bar i})u_t^i(y^i,x^i)$了。</p> </blockquote> <h3 id="分解条件速率和条件概率路径">分解条件速率和条件概率路径</h3> <p>接下来我们还需要分解条件速率和条件概率路径。换句话说，我们希望得到分解版的离散边缘化技巧（Discrete Factorized Marginalization Trick）。</p> <p><strong>定理16</strong>（Discrete Factorized Marginalization Trick）考虑一个边缘概率路径通过如下方式构造：</p> \[p_t(x) =\sum_z p_{t\mid Z}(x \mid z) p_Z(z)\text{, with }p_{t\mid Z}(x\mid z)= \prod_ip_{t\mid Z}^i(x^i\mid z)\tag{10}\] <p>其中，$p_{t\mid Z}^i(x^i\mid z)$是分解条件概率路径，就像公式（6）中的分解边缘概率路径一样。</p> <p>假设：$\forall x \in \mathcal{S}, \forall t\in[0,1)$， $p_t(x) &gt; 0$。<strong>分解条件速率$u_t^{i}(y^i, x^i \mid z) \in C([0,1))$<font color="red">生成</font>分解条件概率路径$p_{t\mid Z}^i(x^i\mid z) \in C^1([0,1))$。</strong></p> <p><strong>假设满足时，边缘速率</strong></p> \[u_t(y,x) =\sum_i\delta(y^{\bar i},x^{\bar i})u_t^i(y^i,x)\tag{11}\] <p><strong><font color="red">生成</font>边缘概率路径$p_t(x)$。</strong></p> <p>其中</p> \[u_t^i(y^i,x)=\sum_z u_t^i(y^i,x^i\mid z)p_{Z\mid t}(z\mid x) = \mathbb{E} \left[ u_t^i(y^i,X_t^i\mid Z) \mid X_t = x \right]\tag{12}\] <blockquote> <p>定理16的证明如下</p> \[\begin{aligned} u_t(y,x) &amp;\overset{(*)}{=}\sum_z u_t(y,x\mid z)p_{Z\mid t}(z\mid x)\\ &amp;\overset{公式(7)}{=}\sum_z \left[ \sum_i\delta(y^{\bar i},x^{\bar i})u_t^i(y^i,x^i\mid z) \right] p_{Z\mid t}(z\mid x)\\ &amp;\overset{交换求和顺序}{=}\sum_i \delta(y^{\bar i},x^{\bar i}) \underbrace{\left[ \sum_z u_t^i(y^i,x^i\mid z) p_{Z\mid t}(z\mid x) \right]}_{u_t^i(y^i,x)} \\ \end{aligned} \tag{13}\] <p>（*）步是我们在DFM上一节<sup id="fnref:4"><a href="#fn:4" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>中已经得到的结论。</p> <p>那么根据（13）式，就知道公式（11）和（12）了。</p> <p>接下来我们还要说明<strong>公式（11）表示的边缘速率能够<font color="red">生成</font>边缘概率路径$p_t(x)$</strong>，这需要用到我们上一节<sup id="fnref:4:1"><a href="#fn:4" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>讲的离散边缘化技巧，离散边缘化技巧有使用的假设，即 $p_{t\mid Z}(x\mid z)\in C^{1}([0,1))$，$u_t(y,x\mid z)\in C([0,1))$。我们接下来说明这个假设能满足。</p> <p>根据定理16的假设可知$p_{t\mid Z}^i(x^i\mid z) \in C^1([0,1))$，$\forall t \in C([0,1)), p_t(x) &gt; 0$，所以根据$p_{t\mid Z}(x\mid z)=\prod_ip_{t\mid Z}^i(x^i\mid z)$可知$p_{t\mid Z}(x\mid z)\in C^1([0,1))$。根据定理16的假设可知$u_t^{i}(y^i, x^i \mid z) \in C([0,1))$，根据公式7我们知道$u_t(y,x\mid z)=\sum_i\delta(y^{\bar i},x^{\bar i})u_t^i(y^i,x^i\mid z)$，所以可以知道$u_t(y,x\mid z)\in C([0,1))$。</p> <p>所以离散边缘化技巧假设满足，可以应用该技巧，得出</p> <p><strong>公式（11）表示的边缘速率$u_t(y,x)$能够<font color="red">生成</font>边缘概率路径$p_t(x)$</strong>。</p> <p>所以，定理16得到证明。</p> </blockquote> <h3 id="分解条件损失函数">分解条件损失函数</h3> <p>我们可以让模型预测分解形式的速率$u_t^{\theta,i}$，替代原先预测非分解形式速率$u_t^{\theta}$的形式，那么条件损失函数的形式可以改写为：</p> \[L_{CDFM}(\theta) = \mathbb{E}_{t,Z,X_t\sim p_{t\mid Z}} \sum_i D_{X_t}^i \left( u_t^i(\cdot,X_t \mid Z), u_t^{\theta,i}(\cdot, X_t) \right) \tag{14}\] <p>其中，$t\in U[0,1]$，$u_t^{i}(\cdot,x\mid z), u_t^{\theta,i}(\cdot,x) \in \mathbb{R}^{\mathcal{T}}$。$u_t^{i}(\cdot,x\mid z),u_t^{\theta,i}(\cdot,x)$需要满足速率条件。$D_{x}^i(u,v)$是分解后的Bregman散度。</p> <blockquote> <p>关于$u_t^{i}(\cdot,x\mid z),u_t^{\theta,i}(\cdot,x)$满足速率条件，可以用更形式化的方式表达：</p> <p>定义： 对于$\alpha \in \mathcal{T}$，</p> \[\Omega_{\alpha} = \left\{ v \in \mathbb{R}^{\mathcal{T}}\mid v(\beta)\ge 0 \text{ }\forall\beta \in \mathcal{T}\backslash \{\alpha\}\text{, and } v(\alpha) = -\sum_{\beta\neq \alpha} v(\beta) \right\} \subset \mathbb{R}^{\mathcal{T}}\tag{15}\] <p>$\Omega_{\alpha}$显然是个凸集合。</p> <p>形式化的说，$u_t^{i}(\cdot,x\mid z),u_t^{\theta,i}(\cdot,x)$满足速率条件，就是$u_t^{i}(\cdot,x\mid z),u_t^{\theta,i}(\cdot,x)\in \Omega_{x^i}$。</p> <p>关于$D_{x}^i(u,v)$，在<sup id="fnref:4:2"><a href="#fn:4" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>中说过，它需要一个凸函数来定义。不过相比于非分解的$D_x(u,v)$，$D_{x}^i(u,v)$需要的也是一个分解的凸函数，可以记为$\Phi_x^i : \Omega_{x^i} \to \mathbb R$。</p> </blockquote> <hr/> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>Lipman Y, Havasi M, Holderrieth P, et al. Flow Matching Guide and Code[J]. arXiv preprint arXiv:2412.06264, 2024. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2"> <p>Lipman Y, Chen R T Q, Ben-Hamu H, et al. Flow matching for generative modeling[J]. arXiv preprint arXiv:2210.02747, 2022. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:3"> <p>Campbell A, Benton J, De Bortoli V, et al. A continuous time framework for discrete denoising models[J]. Advances in Neural Information Processing Systems, 2022, 35: 28266-28279. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:4"> <p><a href="https://zhuanlan.zhihu.com/p/16493879333">https://zhuanlan.zhihu.com/p/16493879333</a> <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:4:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:4:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="Flow_Matching"/><category term="Discrete_Flow_Matching"/><category term="formatting"/><category term="math"/><summary type="html"><![CDATA[接着上一次学习的DFM的上半部分，这次学习DFM的下半部分。主要还是基于Flow Matching Guide and Code1。 分解路径、速度和损失 在实际实现DFM的时候，我们会想要像FM2一样用神经网络预测上一节说的条件速度场，即$u^{\theta}_t(y,x) = \mathbb{E}[u_t(y,X_t\mid Z )\mid X_t=x )]$。但是，如果这样做，那么神经网络需要处理非常多种情况。具体而言，因为$y\in \mathcal{S}=\mathcal{T}^d$，其中$\mathcal{T} = {1,\cdots, K}$是vocabulary，$d$是句子长度，所以神经网络输出的大小为$K^d$（对第一个词要考虑$K$个状态，对第二个、第三个直到第$d$个词都要考虑$K$个状态，所以是$K^d$）。这么大的输出大小是不可实现的。为解决这个问题，可以使用分解（factorized）3方法。 下面要逐个使用这种方法来分解（边缘/条件）路径、（边缘/条件）速度和条件损失。 分解边缘速度 分解方法的核心思想在于：只考虑最多一个token的改变。因为多个token的改变可以视为逐个token的单步变化的叠加。探讨一个token的改变就像探讨基变换一样。 分解边缘速度的具体形式是： \[u_t(y,x) = \sum_i \delta(y^{\bar{i}},x^{\bar{i}})u_t^i(y^i,x)\tag{1}\] $\bar i={1,\cdots, i-1,i+1,\cdots,d}$，就是把$i$去掉，相当于只考虑第$i$个token的变化。分解边缘速度如下图所示： 分解边缘速度图示 现在我们只需要使用模型预测$u_t^{i}(y^i,x)$了。其中，$i\in [d] = {1,\cdots,d}$，$y^i \in \mathcal{T}$是第$i$个token状态改变后所在的状态，$x\in \mathcal{S}$为状态转移之前的全部token的状态，$u_t^i(y^i,x) \in \mathbb R$是一个标量速率。所以，模型要输出的结果的维度是$d\cdot K$（总共$d$个token，每个token有$K$种可以改变到的状态，对应$d\cdot K$种速率），相比之前的$K^d$大大减小。 分解的边缘速率还是要满足速率条件，不过是要按照每个token的层级进行描述，如下： \[u_t^i(y^i,x)\ge 0 \text{ for all } y^i \neq x^i \text{, and } \sum_{y^i \in \mathcal{T}}{u^i_t(y^i,x)} = 0 \quad \text{for all }x \in \mathcal{S} \tag{2}\] $\forall i\in [d]$，公式（2）都要满足。 既然我们有了分解边缘概率，那么采样过程也可以coordinate-wise（逐维）地进行，代入公式（1）到CTMC模型的转移核之中，即： \[\begin{aligned} \mathbb{P}(X_{t+h}=y\mid X_t=x)&amp;=\delta(y,x)+hu_t(y,x)+o(h)\\ &amp;\overset{代入公式(1)}{=} \delta(y,x)+h\sum_i \delta(y^{\bar{i}},x^{\bar{i}})u_t^i(y^i,x)+o(h)\\ &amp;\overset{公式(4)}{=} \prod_{i}\left[ \delta(y^i,x^i) + h u_t^i(y^i,x) +o(h) \right] \end{aligned} \tag{3}\] 公式（3）第二个等号基于如下等式： \[\prod_i\left[ a^i + h b^i \right] = \prod_i a^i + h\sum_{i}\left(\prod_{j\neq i}a^j\right) b^i + o(h) \tag{4}\] 取$a^i = \delta(y^i,x^i)$，$b^i = u_t^i(y^i,x)$，同时注意到$\prod_i \delta(y^i,x^i) = \delta(y,x)$，就得出了公式（3）中第二个等号表示的关系。 所以我们还可以用分解边缘速率定义一个分解转移核用以采样： \[\begin{aligned} \mathbb{P}(X^i_{t+h}=y^i\mid X_t=x) &amp;=\delta(y^i,x^i) + h u_t^i(y^i,x) +o(h) \\ \mathbb{P}(X_{t+h}=y\mid X_t=x) &amp;=\prod_i\mathbb{P}(X^i_{t+h}=y^i\mid X_t=x) \end{aligned}\tag{5}\] 采样方法还是可以用CTMC模型里面说过的Euler法，但是这里是per coordinate的采样，即每一维单独进行采样。 其实FM的采样也可以写成类似的分解的形式，但一般不这么用。值得一提的是：FM的采样是确定性的，而DFM的采样具有随机性。 分解边缘概率路径 上面我们已经实现了分解边缘速度，现在考虑用一样的方法分解边缘概率路径$q_t(x)$： \[q_t(x) = \prod_{i}q_t^i(x^i)\tag{6}\] 其中，$q_t^i(x^i)$就是分解后的边缘概率路径。 接下来我们想说明：分解边缘速率生成分解边缘概率路径时，非分解的边缘速率也能生成非分解的边缘概率路径，也就是希望边缘速率和边缘概率路径的关系在分解和非分解形式下都保留。具体而言就是如下命题： 命题2 令$q_t(x)$由公式（6）所示的分解形式定义，当$u_t^i(y^i,x^i)\in C([0,1))$能够生成$q_t^i(x^i)$时，那么$q_t$由如下分解形式的边缘速率生成： \[u_t(y,x)=\sum_i\delta(y^{\bar i},x^{\bar i})u_t^i(y^i,x^i) \tag{7}\] 命题2的证明如下： 首先根据边缘概率的定义，我们可以得到$x^i$和$x^{\bar i}$的边缘概率可以通过如下方式计算（就是把无关的变量求和掉）： \[q^i(x^i) := \sum_{x^{\bar i}}q(x) \quad q^{\bar i }(x^{\bar i }) :=\sum_{x^i} q(x) \tag{8}\] 推导如下： \[\begin{aligned} \cfrac{\mathrm{d}}{\mathrm{d}t}q_t(y) &amp;\overset{代入公式(6)}{=} \cfrac{\mathrm{d}}{\mathrm{d}t}\prod_i q_t^i(y^i)\\ &amp;\overset{乘法求导法则}{=} \sum_i q^{\bar i}_t(y^{\bar i}) \cfrac{\mathrm{d}}{\mathrm{d}t}q_t^i(y^i)\\ &amp;\overset{\text{Kolmogorov}方程}{=} \sum_i q^{\bar i}_t(y^{\bar i}) \left[ \sum_{x^i} u_t^i(y^i,x^i)q_t^i(x^i) \right]\\ &amp;\overset{(*)}{=} \sum_i \left[ \sum_{x^{\bar i}}\delta(y^{\bar i}, x^{\bar i} ) q_t^{\bar i}(x^{\bar i}) \right] \left[ \sum_{x^i} u_t^i(y^i,x^i)q_t^i(x^i) \right]\\ &amp;\overset{分配律}{=} \sum_i \sum_{x^{\bar i}}\sum_{x^i} \left[ \delta(y^{\bar i}, x^{\bar i} ) q_t^{\bar i}(x^{\bar i}) u_t^i(y^i,x^i)q_t^i(x^i) \right]\\ &amp;\overset{合并x^i和x^{\bar i}}{=} \sum_i \sum_{x} \left[ \delta(y^{\bar i}, x^{\bar i} ) u_t^i(y^i,x^i)q_t(x) \right]\\ &amp;\overset{交换求和顺序}{=} \sum_{x}\left[\sum_i \left[ \delta(y^{\bar i}, x^{\bar i} ) u_t^i(y^i,x^i) \right]q_t(x)\right]\\ \end{aligned} \tag{9}\] （*）式基于$q_t^{\bar i }(y^{\bar i}) = \sum_{x^{\bar i}}\delta(y^{\bar i}, x^{\bar i})q_t^{\bar i}(x^{\bar i})$。根据最后结果，对比Kolmogorov方程，就知道$u_t(y,x)=\sum_i\delta(y^{\bar i},x^{\bar i})u_t^i(y^i,x^i)$了。 分解条件速率和条件概率路径 接下来我们还需要分解条件速率和条件概率路径。换句话说，我们希望得到分解版的离散边缘化技巧（Discrete Factorized Marginalization Trick）。 定理16（Discrete Factorized Marginalization Trick）考虑一个边缘概率路径通过如下方式构造： \[p_t(x) =\sum_z p_{t\mid Z}(x \mid z) p_Z(z)\text{, with }p_{t\mid Z}(x\mid z)= \prod_ip_{t\mid Z}^i(x^i\mid z)\tag{10}\] 其中，$p_{t\mid Z}^i(x^i\mid z)$是分解条件概率路径，就像公式（6）中的分解边缘概率路径一样。 假设：$\forall x \in \mathcal{S}, \forall t\in[0,1)$， $p_t(x) &gt; 0$。分解条件速率$u_t^{i}(y^i, x^i \mid z) \in C([0,1))$生成分解条件概率路径$p_{t\mid Z}^i(x^i\mid z) \in C^1([0,1))$。 假设满足时，边缘速率 \[u_t(y,x) =\sum_i\delta(y^{\bar i},x^{\bar i})u_t^i(y^i,x)\tag{11}\] 生成边缘概率路径$p_t(x)$。 其中 \[u_t^i(y^i,x)=\sum_z u_t^i(y^i,x^i\mid z)p_{Z\mid t}(z\mid x) = \mathbb{E} \left[ u_t^i(y^i,X_t^i\mid Z) \mid X_t=x \right]\tag{12}\] 定理16的证明如下 \[\begin{aligned} u_t(y,x) &amp;\overset{(*)}{=}\sum_z u_t(y,x\mid z)p_{Z\mid t}(z\mid x)\\ &amp;\overset{公式(7)}{=}\sum_z \left[ \sum_i\delta(y^{\bar i},x^{\bar i})u_t^i(y^i,x^i\mid z) \right] p_{Z\mid t}(z\mid x)\\ &amp;\overset{交换求和顺序}{=}\sum_i \delta(y^{\bar i},x^{\bar i}) \underbrace{\left[ \sum_z u_t^i(y^i,x^i\mid z) p_{Z\mid t}(z\mid x) \right]}_{u_t^i(y^i,x)} \\ \end{aligned} \tag{13}\] （*）步是我们在DFM上一节4中已经得到的结论。 那么根据（13）式，就知道公式（11）和（12）了。 接下来我们还要说明公式（11）表示的边缘速率能够生成边缘概率路径$p_t(x)$，这需要用到我们上一节4讲的离散边缘化技巧，离散边缘化技巧有使用的假设，即 $p_{t\mid Z}(x\mid z)\in C^{1}([0,1))$，$u_t(y,x\mid z)\in C([0,1))$。我们接下来说明这个假设能满足。 根据定理16的假设可知$p_{t\mid Z}^i(x^i\mid z) \in C^1([0,1))$，$\forall t \in C([0,1)), p_t(x) &gt; 0$，所以根据$p_{t\mid Z}(x\mid z)=\prod_ip_{t\mid Z}^i(x^i\mid z)$可知$p_{t\mid Z}(x\mid z)\in C^1([0,1))$。根据定理16的假设可知$u_t^{i}(y^i, x^i \mid z) \in C([0,1))$，根据公式7我们知道$u_t(y,x\mid z)=\sum_i\delta(y^{\bar i},x^{\bar i})u_t^i(y^i,x^i\mid z)$，所以可以知道$u_t(y,x\mid z)\in C([0,1))$。 所以离散边缘化技巧假设满足，可以应用该技巧，得出 公式（11）表示的边缘速率$u_t(y,x)$能够生成边缘概率路径$p_t(x)$。 所以，定理16得到证明。 分解条件损失函数 我们可以让模型预测分解形式的速率$u_t^{\theta,i}$，替代原先预测非分解形式速率$u_t^{\theta}$的形式，那么条件损失函数的形式可以改写为： \[L_{CDFM}(\theta) = \mathbb{E}_{t,Z,X_t\sim p_{t\mid Z}} \sum_i D_{X_t}^i \left( u_t^i(\cdot,X_t \mid Z), u_t^{\theta,i}(\cdot, X_t) \right) \tag{14}\] 其中，$t\in U[0,1]$，$u_t^{i}(\cdot,x\mid z), u_t^{\theta,i}(\cdot,x) \in \mathbb{R}^{\mathcal{T}}$。$u_t^{i}(\cdot,x\mid z),u_t^{\theta,i}(\cdot,x)$需要满足速率条件。$D_{x}^i(u,v)$是分解后的Bregman散度。 关于$u_t^{i}(\cdot,x\mid z),u_t^{\theta,i}(\cdot,x)$满足速率条件，可以用更形式化的方式表达： 定义： 对于$\alpha \in \mathcal{T}$， \[\Omega_{\alpha} = \left\{ v \in \mathbb{R}^{\mathcal{T}}\mid v(\beta)\ge 0 \text{ }\forall\beta \in \mathcal{T}\backslash \{\alpha\}\text{, and } v(\alpha) = -\sum_{\beta\neq \alpha} v(\beta) \right\} \subset \mathbb{R}^{\mathcal{T}}\tag{15}\] $\Omega_{\alpha}$显然是个凸集合。 形式化的说，$u_t^{i}(\cdot,x\mid z),u_t^{\theta,i}(\cdot,x)$满足速率条件，就是$u_t^{i}(\cdot,x\mid z),u_t^{\theta,i}(\cdot,x)\in \Omega_{x^i}$。 关于$D_{x}^i(u,v)$，在4中说过，它需要一个凸函数来定义。不过相比于非分解的$D_x(u,v)$，$D_{x}^i(u,v)$需要的也是一个分解的凸函数，可以记为$\Phi_x^i : \Omega_{x^i} \to \mathbb R$。 Lipman Y, Havasi M, Holderrieth P, et al. Flow Matching Guide and Code[J]. arXiv preprint arXiv:2412.06264, 2024. &#8617; Lipman Y, Chen R T Q, Ben-Hamu H, et al. Flow matching for generative modeling[J]. arXiv preprint arXiv:2210.02747, 2022. &#8617; Campbell A, Benton J, De Bortoli V, et al. A continuous time framework for discrete denoising models[J]. Advances in Neural Information Processing Systems, 2022, 35: 28266-28279. &#8617; https://zhuanlan.zhihu.com/p/16493879333 &#8617; &#8617;2 &#8617;3]]></summary></entry><entry><title type="html">离散型流匹配模型（1）</title><link href="https://ly403.github.io/blog/2025/%E7%A6%BB%E6%95%A3%E6%B5%81%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-1/" rel="alternate" type="text/html" title="离散型流匹配模型（1）"/><published>2025-01-05T00:00:00+00:00</published><updated>2025-01-05T00:00:00+00:00</updated><id>https://ly403.github.io/blog/2025/%E7%A6%BB%E6%95%A3%E6%B5%81%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B%EF%BC%881%EF%BC%89</id><content type="html" xml:base="https://ly403.github.io/blog/2025/%E7%A6%BB%E6%95%A3%E6%B5%81%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-1/"><![CDATA[<p>本文是接着上一次学习的CTMC模型继续学习离散型流匹配模型，主要是读Flow Matching Guide and Code<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>第7章的笔记。并非完全Flow Matching Guide and Code的翻译，挑了一些重要的内容并且加入了我自己的的理解。</p> <p>要构建一个离散型流匹配模型（Discrete Flow Matching，DFM），分为三个步骤：</p> <ol> <li>类比连续型FM，定义一个概率路径$p_t$，连接原始分布的pmf和目标分布的pmf。pmf就是概率质量函数。</li> <li>构建一个CTMC模型，\({(X_{t})}_{0 \le t \le 1}\)，该CTMC的速度场\(u_{t}^{\theta}\)生成出第1步中的概率路径\(p_t\)，\(\theta\)是可学习的参数。</li> <li>训练\(u_{t}^{\theta}\)，使之最小化Bregman散度，Bregman散度就是DFM的损失函数。</li> </ol> <p>后面我就把离散型流匹配模型简称为DFM，连续型流匹配模型简称为FM。</p> <h2 id="数据和数据的成对">数据和数据的成对</h2> <p>像FM里面一样，定义源分布的pmf为$p$，目标分布的pmf为$q$。源分布的随机变量（Random Variable，RV）记为$X_0$，目标分布的随机变量记为$X_1$。$X_0,X_1 \in \mathcal{S}$，$\mathcal{S}$就是CTMC模型里面提到的状态空间。</p> <p>考虑到我们一般把$X_0$和$X_1$放在一起讨论，所以需要找个方式表示$X_0$和$X_1$的成对分布：$(X_0,X_1)\sim p(X_0)q(X_1)$，就是当成两个独立的RV，用二者的pmf的乘积表示$(X_0,X_1)$的联合分布。也可以用单独的符号$\pi_{0,1}(x_0,x_1)$表示$p(x_0)$和$q(x_1)$的联合分布。</p> <p>举个例子理解，如果对文本生成任务，源分布$p(x_0)$就是先验分布，一般取：</p> <ol> <li> <p>$\mathcal{S}$上的均匀分布</p> </li> <li> <p>加入一个mask的特殊token $\mathtt{m}$到vocabulary $\mathcal{T}$里面，即$\mathcal{T}\cup {\mathtt{m}}$。</p> </li> </ol> <p>按第2种方法，此时得到$\pi_{0,1}(x_0,x_1)=\delta(x_0,\mathtt{m})q(x_1)$，先验分布就是$\delta(x_0,\mathtt{m})$。若$X_0 \sim \delta(X_0,\mathtt{m})$，则$X_0=(\mathtt{m},\cdots,\mathtt{m})$。</p> <h2 id="离散概率路径">离散概率路径</h2> <p>FM<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>在研究边缘概率路径$p_t(x)$和生成$p_t(x)$的边缘向量场时$u_t(x)$遇到困难（图像在高维空间中造成<sup id="fnref:2:1"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>中的公式6和8的积分不可解，因此没办法得到$p_t(x)$和$u_t(x)$的解析式），所以转而求解条件向量场$u_t(x\mid z)$和条件概率路径$p_t(x\mid z)$。仿照这个办法，DFM也可以按如下方式算边缘概率路径（marginal probability path）：</p> \[p_t(x) = \sum_{z\in \mathcal{Z}}{p_{t\mid Z}(x\mid z)p_{Z}(z)}\tag{1}\] <p>其中$Z\in \mathcal{Z}$是作为条件的随机变量，$\mathcal{Z}$是任意空间。上式是离散版的全概率公式。需要指出的是，<strong>条件概率路径$p_{t\mid Z}(x\mid z)$必须满足边界约束$p_0 = p$和$p_1=q$，</strong>这样才能构建源分布和目标分布之间的关系。</p> <h2 id="边缘化技巧">边缘化技巧</h2> <p>我们在公式1中已经给出了边缘概率路径$p_t(x)$和条件概率路径$p_{t\mid Z}(x\mid z)$之间的关系，类比在FM里面做的事情，对DFM，我们也想知道条件速度场$u_t(y,x\mid z)$和边缘条件速度场$u_t(y,x)$之间的关系。</p> <p>为了研究这个问题，可以从FM中获得经验。FM中，如果条件向量场$u_t(\cdot\mid z)$能生成条件概率路径$p_{t\mid Z}(\cdot\mid z)$，那边缘向量场$u_t(x)$满足：</p> \[u_t(x) = \int u_t(x\mid z)\cfrac{p_{t\mid Z}(x\mid z)p_Z(z)}{p_t(x)}\mathrm{d} z =\int u_t(x\mid z) p_{Z\mid t}(z\mid x) \mathrm{d}z \tag{2}\] <p>公式2可以参考Flow matching for generative modeling<sup id="fnref:2:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>里面的公式8，或者Flow Matching Guide and Code<sup id="fnref:1:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>里面的公式4.12。</p> <p>类比公式2，是否DFM也存在如下关系：当条件速度场$u_t(\cdot,\cdot\mid z)$生成条件概率路径$p_{t\mid Z}(\cdot\mid z)$时，边缘速度场$u_t(y,x)$满足</p> \[u_t(y,x) = \sum_z u_t(y,x \mid z) \cfrac{p_{t\mid Z}(x\mid z)p_Z(z)}{p_t(x)} =\sum_z u_t(y,x\mid z) p_{Z\mid t}(z\mid x) \tag{3}\] <p>答案是肯定的。接下来详细表述和证明这个问题。</p> <hr/> <p><strong>假设</strong> $p_{t\mid Z}(x\mid z)\in C^{1}([0,1))$。$u_t(y,x\mid z)\in C([0,1))$。对所有$t\in[0,1)$和$x\in \mathcal{S}$，$p_t(x)&gt; 0$。</p> <p><strong>定理</strong> （离散边缘化技巧）当上述假设满足时，如果$u_t(y,x\mid z)$生成$p_{t\mid Z}(x\mid z)$，那么对$t\in[0,1)$，边缘向量场$u_t(y,x)$（公式3）生成边缘概率路径$p_t(x)$（公式1）。</p> <p><strong>证明</strong></p> \[\begin{aligned} \cfrac{\mathrm{d}}{\mathrm{d}t}p_t(y) &amp;\overset{(1)}{=} \cfrac{\mathrm{d}}{\mathrm{d}t}\left[ \sum_z{ p_{t\mid Z}\left(y\mid z\right)p_{Z}(z) }\right]\\ &amp;\overset{(2)}{=} \sum_z{ \cfrac{\mathrm{d}}{\mathrm{d}t}\left[ p_{t\mid Z}\left(y\mid z\right)p_{Z}(z) \right] }\\ &amp;\overset{(3)}{=} \sum_z{ \cfrac{\mathrm{d}}{\mathrm{d}t}\left[ p_{t\mid Z}\left(y\mid z\right) \right]p_{Z}(z) }\\ &amp;\overset{(4)}{=} \sum_z {\sum_x \left[{ u_t(y,x \mid z) p_{t\mid Z}(x\mid z) } \right] p_Z(z)}\\ &amp;\overset{(5)}{=} \sum_x {\sum_z \left[{ u_t(y,x \mid z) \cfrac{p_{t\mid Z}(x\mid z)p_Z(z)}{p_t(x)} } \right] p_t(x)}\\ &amp;\overset{(6)}{=} \sum_x {\overbrace{\sum_z \left[{ u_t(y,x \mid z) p_{Z\mid t}(z\mid x) } \right]}^{u_t(y,x)} p_t(x)} \end{aligned} \tag{4}\] <p>第（4）步是因为根据条件“$u_t(y,x\mid z)$生成$p_{t\mid Z}(x\mid z)$”，我们在CTMC模型中讲到的离散质量守恒Discrete Mass Conservation（Discrete Mass Conservation的假设已经满足了，就是上文给出的假设），可知：</p> <ul> <li>$u_t(y,x\mid z)$生成$p_{t\mid Z}(x\mid z)$</li> <li> <font color="red">$u_t(y,x\mid z)$和$p_{t\mid Z}$满足Kolmogorov方程</font> <p>，且<font color="blue"> $u_t(y,x\mid z)$满足速率条件（rate conditions）</font>。</p> </li> </ul> <p>二者等价，故直接带入Kolmogorov方程可得第（4）步，只是加了个条件$z$而已。</p> <p>第（5）是将$p_Z(z)$拿到内层求和，并在内层求和的分母上添加一个$p_t(x)$，此时还需要再乘$p_t(x)$，可以用分配律把它拿出内层求和。</p> <p>第（6）步是贝叶斯公式。</p> <p>最终得出的等式比对一下Kolmogorov方程，可见$u_t(y,x)=\sum_z u_t(y,x \mid z) p_{Z\mid t}(z\mid x)$和$p_t$满足Kolmogorov方程。</p> <p>然后，$u_t(y,x)$满足速率条件，因为$u_t(y,x \mid z)$满足速率条件，即上文蓝色字的部分。</p> <blockquote> <p>关于为什么$u_t(y,x \mid z)$满足速率条件，$u_t(y,x)$就满足速率条件的证明：</p> <p>如果$u_t(y,x \mid z)$满足速率条件，那么 $\forall y\neq x , u_t(y,x\mid z) \ge 0$且$\sum_y u_t(y,x\mid z)=0$</p> <p>根据$u_t(y,x)=\sum_z u_t(y,x \mid z) p_{Z\mid t}(z\mid x)$，因为$p_{Z\mid t}(z\mid x)$显然不小于0，故$u_t(y,x) \ge 0$。</p> \[\begin{aligned} \sum_y u_t(y,x) &amp;=\sum_y\sum_z u_t(y,x \mid z) p_{Z\mid t}(z\mid x) \\ &amp;=\sum_z\sum_y u_t(y,x \mid z) p_{Z\mid t}(z\mid x) \\ &amp;= \sum_z 0p_{Z\mid t}(z\mid x) \\ &amp;= 0 \end{aligned} \tag{5}\] <p>所以$u_t(y,x)$也满足速率条件。</p> </blockquote> <p>在再次应用Discrete Mass Conservation之前，还得确定一下假设是否满足，因为$u_t(y,x\mid z)\in C([0,1)$且$p_{t\mid Z}(x\mid z)\in C^{1}([0,1))$，故$u_t(y,x)\in C([0,1))$，$p_t(x)\in C^1([0,1))$。</p> <blockquote> <p>关于为什么\(u_t(y,x\mid z)\in C([0,1)\)且\(p_{t\mid Z}(x\mid z)\in C^{1}([0,1))\)，就有\(u_t(y,x)\in C([0,1))\)。我自己的理解如下：</p> <p>因为\(p_t(x) = \sum_{z}{p_{t\mid Z}(x\mid z)p_{Z}(z)}\)，\(p_{t\mid Z}(x\mid z)\)在\(t\in[0,1)\)上连续，\(p_Z(z)\)和\(t\)无关，所以\(p_t(x)\)在\(t\in[0,1)\)连续。\(p_{t\mid Z}(x\mid z)\)的一阶导在\(t\in[0,1)\)上连续，\(p_Z(z)\)和\(t\)无关，\(\dot p_t(x) = \sum_{z}{\dot p_{t\mid Z}(x\mid z)p_{Z}(z)}\)，故\(\dot p_t(x)\)也在\(t\in[0,1)\)上连续。</p> <p>因为$u_t(y,x)=\sum_z u_t(y,x \mid z) p_{Z\mid t}(z\mid x)= \sum_z u_t(y,x \mid z) \cfrac{p_{t\mid Z}(x\mid z)p_Z(z)}{p_t(x)}$，在$t\in[0,1)$时，$p_{t\mid Z}(x\mid z)$和$u_t(y,x\mid z)$都连续，$p_Z(z)$和$t$无关，根据假设$p_t(x)&gt;0$，同时$p_t(x)$在$t\in[0,1)$连续，所以$u_t(y,x)$在$t\in[0,1)$连续。从这里能看到假设为什么一定要$p_t(x)&gt;0$。</p> <p>从上面这些解释就知道$u_t(y,x)\in C([0,1))$，$p_t(x) \in C^1([0,1)$。</p> </blockquote> <p>所以假设也满足，可以应用Discrete Mass Conservation，得到<strong>$u_t(y,x)$生成$p_{t}(x)$</strong>。证明完毕。</p> <p><strong>补充</strong> 关于$t\in[0,1)$时，$p_t&gt;0$的假设是可以满足的。因为我们可以使用$(1-(1-t)\epsilon) \cdot p_{Z\mid t} + (1 - t)\epsilon \cdot p_{\text{uni}}$，其中$p_{\text{uni}}$是均匀分布，$\epsilon$是大于0的很小的值。我的理解就是通过稍微加入一点服从均匀分布的噪声，让$[0,1)$上，$p_t$都有概率。</p> <hr/> <h2 id="dfm损失">DFM损失</h2> <p>像FM里面一样，我们可以用一个神经网络来学习速度场$u_{t}^{\theta}(y,x)$，其中$\theta$是可学习的参数。类比FM，可以构建DFM的损失：</p> \[L_{DFM}(\theta) = \mathbb{E}_{t,X_t\sim p_t}D_{X_t}\big(u_t(\cdot,X_t),u_t^{\theta}(\cdot,X_t)\big) \tag{6}\] <p>这里，$t\sim U[0,1]$，$u_t(\cdot,x)\in R^{\mathcal{S}}$满足速率条件。也就是说，$u_t(\cdot,x)\in \Omega_x$，且：</p> \[\Omega_x = \left\{ v \in R^{\mathcal{S}} \mid v(y) &gt; 0 \text{ }\forall y \neq x,\text{ and } v(x) = - \sum_{y\neq x} v(y) \right\} \in R^{\mathcal{S}} \tag{7}\] <p>$\Omega_x$是凸集合。$D_{X_t}(u,v)$是Bregman散度，其使用凸函数$\Phi_x : \Omega_x \to \mathbb{R}$定义。</p> <hr/> <p>Bregman的具体计算方式如下图和下式所示：</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post/DFM1-480.webp 480w,/assets/img/post/DFM1-800.webp 800w,/assets/img/post/DFM1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post/DFM1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Bregman的具体计算方式图示 </div> \[\displaystyle D( u,v) \ :=\ \Phi ( u) \ -\ [ \Phi ( v) \ +\ \langle u-v,\nabla \Phi ( v) \rangle ] \tag{8}\] <p>Bregman散度有一些关键的性质，在FM和DFM里面最关键的是它对第二个参数的梯度有仿射不变性（affine invariant）<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>：</p> \[\nabla_v D(au_1 + b_u2, v) = a\nabla_v D(u_1,v) +b\nabla_vD(u_2,v), \text{ for any } a+b=1 \tag{9}\] <p>因为仿射不变性，可以得到$\nabla_vD(\mathbb{E}[Y] ,v) =\mathbb{E}[\nabla_v D(Y,v)]$。</p> <p>关于Bregman散度的详细信息，在知乎上已经有很好的回答了，可以参考：https://www.zhihu.com/question/22426561。</p> <hr/> <p>就像在FM里面一样，我们更关注条件DFM（Conditional Discrete Flow Matching，CDFM）的损失函数形式，因为我们想仿照FM里面一样用神经网络预测条件速度场，如Flow matching for generative modeling<sup id="fnref:2:3"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>里面的公式9所示。那么CDFM的损失就是：</p> \[L_{CDFM}(\theta) = \mathbb{E}_{t,Z,X_t\sim p_{t\mid Z}}D_{X_t}\big(u_t(\cdot,X_t \mid Z),u_t^{\theta}(\cdot,X_t)\big) \tag{10}\] <p>$u_t(\cdot,X_t \mid Z)$是目标条件速度场，$u_t^{\theta}(\cdot,X_t)$是模型学习的条件速度场。</p> <p>像FM里面一样，我们也能说明公式6和公式10的梯度一样：</p> <p><strong>定理</strong> DFM和CDFM损失的梯度相同：</p> \[\nabla_\theta L_{DFM}(\theta) =\nabla_\theta L_{CDFM}(\theta) \tag{11}\] <p>使得CDFM损失最小的$u^{\theta}_t(y, X_t)$是：</p> \[u^{\theta}_t(y,X_t) = \mathbb{E} \left[ u_t(y,X_t \mid Z) \mid X_t =x \right] \tag{12}\] <hr/> <p>我参照<sup id="fnref:1:2"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>以及一些自己的理解给了<strong>证明</strong>。</p> <p>先看公式11的证明：</p> \[\begin{aligned} \nabla_\theta L_{DFM}(\theta) &amp;= \nabla_\theta \mathbb{E}_{t,X_t\sim p_t}\left[D_{X_t}\big(u_t(y,X_t),u_t^{\theta}(y,X_t)\big)\right] \\ &amp;= \mathbb{E}_{t,X_t\sim p_t} \left[\nabla_\theta D_{X_t}\big(u_t(y,X_t),u_t^{\theta}(y,X_t)\big) \right]\\ &amp;= \mathbb{E}_{t,X_t\sim p_t} \left[\nabla_{u^{\theta}_t(y,X_t)} D_{X_t}\big(u_t(y,X_t),u_t^{\theta}(y,X_t)\big)\nabla_{\theta}u^{\theta}_t(y,X_t) \right]\\ &amp;=\mathbb{E}_{t,X_t\sim p_t} \left[\nabla_{u^{\theta}_t(y,X_t)} D_{X_t}\big( \sum_Z u_t(y,X_t \mid Z) p_{Z\mid t}(Z\mid X_t) ,u_t^{\theta}(y,X_t)\big) \nabla_{\theta}u^{\theta}_t(y,X_t) \right] \\ &amp;=\mathbb{E}_{t,X_t\sim p_t} \left[\nabla_{u^{\theta}_t(y,X_t)} D_{X_t}\left( \mathbb{E}_{Z\sim p_{Z\mid t}(Z\mid X_t)} \left[u_t(y,X_t \mid Z)\right] ,u_t^{\theta}(y,X_t) \right) \nabla_{\theta}u^{\theta}_t(y,X_t) \right] \\ &amp;=\mathbb{E}_{t,X_t\sim p_t} \left[\nabla_{u^{\theta}_t(y,X_t)} \mathbb{E}_{Z\sim p_{Z\mid t}(Z\mid X_t)} \left[ D_{X_t}\left( u_t(y,X_t \mid Z) ,u_t^{\theta}(y,X_t) \right) \nabla_{\theta}u^{\theta}_t(y,X_t)\right] \right] \\ &amp;=\mathbb{E}_{t,X_t\sim p_t} \left[ \mathbb{E}_{Z\sim p_{Z\mid t}(Z\mid X_t)} \left[ \nabla_{u^{\theta}_t(y,X_t)} D_{X_t}\left( u_t(y,X_t \mid Z) ,u_t^{\theta}(y,X_t) \right) \nabla_{\theta}u^{\theta}_t(y,X_t)\right] \right] \\ &amp;=\mathbb{E}_{t,X_t\sim p_t} \left[ \mathbb{E}_{Z\sim p_{Z\mid t}(Z\mid X_t)} \left[ \nabla_{\theta} D_{X_t}\left( u_t(y,X_t \mid Z) ,u_t^{\theta}(y,X_t) \right) \right] \right] \\ &amp;=\nabla_{\theta} \mathbb{E}_{t,X_t\sim p_t} \left[ \mathbb{E}_{Z\sim p_{Z\mid t}(Z\mid X_t)} \left[ D_{X_t}\left( u_t(y,X_t \mid Z) ,u_t^{\theta}(y,X_t) \right) \right] \right] \\ &amp;=\nabla_{\theta} \mathbb{E}_{t,X_t\sim p_{t\mid Z}(X_t \mid Z),Z\sim q(Z)} \left[ D_{X_t}\left( u_t(y,X_t \mid Z) ,u_t^{\theta}(y,X_t) \right) \right] \\ &amp;= \nabla_{\theta} L_{CDFM}(\theta) \end{aligned} \tag{13}\] <p>再看公式12的证明：</p> <p>代入公式12到$L_{CDFM}(\theta)$中，得到</p> \[L_{CDFM}(\theta) = \mathbb{E}_{t,Z,X_t\sim p_{t\mid Z}}D_{X_t}\Big(u_t(y,X_t \mid Z), \mathbb{E} \left[ u_t(y,X_t \mid Z) \mid X_t =x \right]\Big) \tag{14}\] <p>按照Bregman散度的定义，可知$L_{CDFM}(\theta) =0$，是一个全局最小值。所以使得$L_{CDFM}(\theta)$最小的$u^{\theta}_t$满足公式12。</p> <p>在Flow Matching Guide and Code<sup id="fnref:1:3"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>21页最上面（公式4.25和4.26）还有一个更一般化的证明。</p> <hr/> <p>这一节先到这里吧。</p> <hr/> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>Lipman Y, Havasi M, Holderrieth P, et al. Flow Matching Guide and Code[J]. arXiv preprint arXiv:2412.06264, 2024. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:1:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:1:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a> <a href="#fnref:1:3" class="reversefootnote" role="doc-backlink">&#8617;<sup>4</sup></a></p> </li> <li id="fn:2"> <p>Lipman Y, Chen R T Q, Ben-Hamu H, et al. Flow matching for generative modeling[J]. arXiv preprint arXiv:2210.02747, 2022. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:2:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:2:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a> <a href="#fnref:2:3" class="reversefootnote" role="doc-backlink">&#8617;<sup>4</sup></a></p> </li> <li id="fn:3"> <p>Holderrieth P, Havasi M, Yim J, et al. Generator Matching: Generative modeling with arbitrary Markov processes[J]. arXiv preprint arXiv:2410.20587, 2024. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="Flow_Matching"/><category term="Discrete_Flow_Matching"/><category term="formatting"/><category term="math"/><summary type="html"><![CDATA[本文是接着上一次学习的CTMC模型继续学习离散型流匹配模型，主要是读Flow Matching Guide and Code1第7章的笔记。并非完全Flow Matching Guide and Code的翻译，挑了一些重要的内容并且加入了我自己的的理解。 Lipman Y, Havasi M, Holderrieth P, et al. Flow Matching Guide and Code[J]. arXiv preprint arXiv:2412.06264, 2024. &#8617;]]></summary></entry><entry><title type="html">连续时间的马可夫链</title><link href="https://ly403.github.io/blog/2025/%E8%BF%9E%E7%BB%AD%E6%97%B6%E9%97%B4%E7%9A%84%E9%A9%AC%E5%8F%AF%E5%A4%AB%E9%93%BE/" rel="alternate" type="text/html" title="连续时间的马可夫链"/><published>2025-01-02T00:00:00+00:00</published><updated>2025-01-02T00:00:00+00:00</updated><id>https://ly403.github.io/blog/2025/%E8%BF%9E%E7%BB%AD%E6%97%B6%E9%97%B4%E7%9A%84%E9%A9%AC%E5%8F%AF%E5%A4%AB%E9%93%BE</id><content type="html" xml:base="https://ly403.github.io/blog/2025/%E8%BF%9E%E7%BB%AD%E6%97%B6%E9%97%B4%E7%9A%84%E9%A9%AC%E5%8F%AF%E5%A4%AB%E9%93%BE/"><![CDATA[<p>本文基于Flow Matching Guide and Code<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> ，主要是读这篇文章时候的笔记。</p> <p>连续时间的马可夫链（Continuous Time Markov Chains，CTMC）是Flow的替代，也是一种生成式模型，用来生成离散的数据。它是离散型Flow Matching（Discrete Flow Matching， DFM）<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>的基础。</p> <h2 id="离散状态空间和随机变量">离散状态空间和随机变量</h2> <p>这一节就是讲一些notations。</p> <p>以类比的方法可以表示离散状态空间：</p> <ul> <li> <p>连续状态空间： $\mathbb{R}^{d}$，$d$是空间维度</p> </li> <li> <p>离散状态空间：$\mathcal{S} = \mathcal{T}^d$，$d$是空间维度，其中$\mathcal{T}=[K]={1,2,\cdots, K}$，$\mathcal{T}$也就是vocabulary。</p> </li> </ul> <p>从$\mathcal{S}$中采样的样本记为$x = (x^1,x^2,\cdots,x^d)\in\mathcal{S}$，其中$x^i$就是一个token。</p> <p>用$X$表示状态空间$\mathcal{S}$中的随机变量，其pmf（probability mass function，概率质量函数，离散分布叫概率质量函数，连续分布叫概率密度函数）为\(p_X : \mathcal{S} \to \mathbb{R}_{\ge 0}\)，实际上pmf是一个从状态空间\(\mathcal{S}\)到正实数集的映射。pmf满足求和为1的约束条件：\(\sum_{x\in\mathcal{S}}p_X(x)=1\)。</p> <p>对于事件$A\in\mathcal{S}$，其发生概率可以如下计算：</p> \[\mathbb{P}(X\in A)=\sum_{x\in A}p_X(x)\] <p>还需要一个delta分布（好像也叫Dirac分布）的pmf表示，如下：</p> \[\delta(x,z)=\begin{cases} +\infty \quad x=z\\ 0\quad \text{else}. \end{cases}\] <p>这个分布表示只有$x$取特定值$z$的时候才有概率，没有概率取其他值。但是该分布的pmf在x的定义域积分也是1。</p> <p>有时候也会在token层级上定义delta分布，例如$\delta(x^i,y^i)\quad x^i,y^i \in \mathcal{T}$。</p> <p>关于Dirac分布的更多信息可以参考：<a href="https://spaces.ac.cn/archives/1870">https://spaces.ac.cn/archives/1870</a></p> <h2 id="ctmc生成式模型">CTMC生成式模型</h2> <p>CTMC是连续时间的马可夫链，马可夫链是离散状态的马可夫过程。所以CTMC是连续时间离散状态的马可夫过程，可以表示为$(X_t)_{0\le t\le 1}$。因为CTMC是随机过程嘛，所以它是一族随机变量。它的马可夫假设可以表示为如下的形式：</p> \[\mathbb{P}(X_{t+s}=z\mid X_s =x,X_u=y,0\le u\le s)=\mathbb{P}(X_{t+s}=z\mid X_s = x)\] <p>类比离散时间的马可夫链，这个公式说的就是只要知道$X_s$的值后，在$s$时刻之前的值（例如式子中的$X_u$），对$s$时候之后的值没有影响（式中$X_{t+s}$）。即“遗忘过去”。关于CTMC更多的介绍可以参考：<a href="https://www.math.pku.edu.cn/teachers/lidf/course/stochproc/stochprocnotes/html/_book/markovc.html#markovc-ctime">https://www.math.pku.edu.cn/teachers/lidf/course/stochproc/stochprocnotes/html/_book/markovc.html#markovc-ctime</a>。</p> <p>类比扩散模型和Flow Matching，也可以定义CTMC的转移核（probability transition kernel）为$p_{t+h\mid t}$，其形式如下：</p> <p>\begin{equation} p_{t+h\mid t}:= \mathbb{P}(X_{t+h}=y\mid X_t = x)=\delta(y,x)+hu_t(y,x)+o(h), \quad \mathbb{P}(X_0=x)=p(x)\label{eq:transition_kernel} \end{equation}</p> <p>$p_{t+h\mid t}$的图示如下：</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post/CTMC-480.webp 480w,/assets/img/post/CTMC-800.webp 800w,/assets/img/post/CTMC-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/post/CTMC.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> CTMC模型的状态转移过程图示 </div> <p>公式$\eqref{eq:transition_kernel}$中，$p$表示随机过程$(X_t)_{0\le t\le 1}$在$t=0$时候的分布（取$t=0$随机过程退化成随机变量）。$o(h)$是高阶无穷小量</p> \[\lim_{h\to0} \cfrac{o(h)}{h} = 0\] <p>其中，$u_t(y,x)$叫做速率（rates或者velocities），它可以类比Flow Matching里面讲的向量场，在CTMC里面表示的是表示状态之间概率转换的速率，按我的理解，就是下面的这个形式：</p> <p>\begin{equation} u_t(y,x) =\lim_{h\to 0}\cfrac{\mathbb{P}(X_{t+h}=y\mid X_t = x)}{h} \label{eq:rate} \end{equation}</p> <p>它是时间$t$的函数。从公式$\eqref{eq:rate}$和公式$\eqref{eq:transition_kernel}$的角度理解，$\delta(y,x)$就是发生状态转移前（即$t$时刻）的初始状态。具体讲，由于我们以$X_t=x$为条件，所以初始状态就是确定的，用概率形式表示就是$\delta(\cdot,x)$，也就是说初始状态时（即$t$时刻），在$x$处有概率，而其他状态空间中的点概率都是0。</p> <p>公式$\eqref{eq:transition_kernel}$中，$u_t(y,x)$需要满足一定的条件，称为速率条件（rate conditions）：</p> <p>\begin{equation} u_t(y,x)\ge 0 \text{ for all } y\neq x \text{, and }\sum_{y}u_t(y,x) =0 \label{eq:rate_cond} \end{equation}</p> <p>这样就是说，$u_t(x,x)&lt; 0$，且$u_t(x,x)=-\sum_{y\neq x}u_t(y,x)$。速度条件的存在是为了保证公式$\eqref{eq:transition_kernel}$算出的转移核$p_{t+h\mid t}(\cdot\mid x)\ge0$且$\sum p_{t+h\mid t}(\cdot\mid x)=1$。</p> <p>类比Flow Matching，我们也可以说：<strong>如果存在$p_{t+h\mid t}$满足公式$\eqref{eq:transition_kernel}$，$p_{t+h\mid t}$的边缘分布为$p_t$，则称$u_t$生成了$p_t$</strong>。</p> <p>用公式$\eqref{eq:transition_kernel}$，我们也可以用Euler法（类比扩散模型和Flow Matching里面ODE的解法）来采样，如下：</p> \[\mathbb{P}(X_{t+h}=y\mid X_t)=\delta(y,X_t)+hu_t(y,X_t)\] <p>但是该式引入了一个偏差，就是$o(h)$。因为这是一个高阶小量，所以只要$h$足够小，那么$o(h)$就会趋于0，这就是希望Euler法采样准确的条件，但是这样采样步骤就会很多。一种缓解这个问题的方法是用如下的Euler法：</p> \[\mathbb{P}(X_{t+h}=y\mid X_t) =\begin{cases} \exp \left[hu_t(X_t,X_t)\right] \qquad \qquad\qquad\qquad\qquad y=X_t \\ \cfrac{u_t(y,X_t)}{\left|u_t(X_t,X_t) \right|}\left( 1 - \exp\left[hu_t(X_t,X_t)\right] \right) \qquad y\neq X_t \end{cases}\] <h2 id="概率路径和kolmogorov方程">概率路径和Kolmogorov方程</h2> <p>类比连续情况下的一致性方程（Continuity Equation），CTMC模型的边缘概率$p_t$满足Kolmogorov方程：</p> <p>\begin{equation} \cfrac{\mathrm{d}}{\mathrm{d}t}p_t(y)=\sum_x u_t(y,x)p_t(x) \label{eq:Kolmogorov} \end{equation}</p> <p>下面这个定理描述了上式这种线性ODE系统唯一解的存在性：</p> <p><strong>定理</strong>（线性ODE的解的存在性和唯一性）：如果$u_t(y,x)$在$C([0,1))$中（即在时间$t$上连续），那么存在唯一的解$p_t(x)$满足Kolmogorov方程$\eqref{eq:Kolmogorov}$，其中，$t\in[0,1)$，初始状态$p_0(x) = p(x)$。</p> <p>对CTMC而言，在$t\in[0,1)$区间，不需要额外的条件，就能够确定解一定存在。关于该定理的更多详细信息可以参考Coddington et al. (1956)<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>中的定理5.1和5.2。</p> <p>方程$\eqref{eq:Kolmogorov}$可以化为：</p> \[\begin{equation} \begin{aligned} \sum_x u_t(y,x)p_t(x) &amp;=\underbrace{\sum_{x\neq y} u_t(y,x)p_t(x)}_{输入通量}-\underbrace{\sum_{x\neq y}u_t(x,y)p_t(y)}_{输出通量} \\ &amp;=-\sum_{x\neq y}\left[j_t(x,y)-j_t(y,x)\right] \\ \end{aligned} \end{equation}\] <p>其中，$j_t(y,x):=u_t(y,x)p_t(x)$，称为概率通量（probability flux）。上式很好理解，就是把从$x$输入到$y$的通量分成了从$x$输入到$y$的部分和从$y$输出到$x$的部分。</p> <p>将输出通量超出来的部分定义为散度，即</p> \[\text{div}(u_t p_t)(y) =\sum_{x\neq y}\left[j_t(x,y)-j_t(y,x)\right]\] <p>那么Kolmogorov方程就可以表示为：</p> \[\dot p_t(y)+\text{div}(u_t p_t)(y)=0\] <p>这和连续型Flow Matching的一致性方程统一起来了。</p> <p>以下还有一个结果，是在CTMC 框架中构建概率路径和速度的主要工具：</p> <p><strong>定理</strong> （离散质量守恒，Discrete Mass Conservation）：$u_t(y,x)$在$C([0,1))$中，$p_t(x)$在$C^1([0,1))$中（我的理解是一阶导数在时间$t$上连续），则如下两个结论是等价的：</p> <ul> <li>在$t\in[0,1)$时， $p_t,u_t$满足Kolmogorov方程$\eqref{eq:Kolmogorov}$，$u_t$满足速率条件$\eqref{eq:rate_cond}$。</li> <li>在$t\in[0,1)$时，$u_t$生成（用前文的术语）了$p_t$。</li> </ul> <p>该定理的证明在<sup id="fnref:1:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>的appendix A.1.。</p> <h3 id="保概率速率probability-preserving-velocities">保概率速率（Probability preserving velocities）</h3> <p>如果$u_t(y,x)$生成了概率路径$p_t(x)$，我们构造一个新的速率：</p> \[\tilde u_t(y,x) = u_t(y,x) + v_t(y,x)\] <p>只要$v_t(y,x)$满足速率条件且</p> \[\sum_x v_t(y,x)p_t(x) = 0\] <p>那么$\tilde u_t(y,x)$还能Kolmogorov方程：</p> \[\sum_x \tilde u_t(y,x)p_t(x) = \sum_x u_t(y,x)p_t(x) = \dot p_t(y)\] <p>那么$\tilde u_t(y,x)$还能生成$p_t(x)$。这个结论在离散型Flow Matching里面非常重要。</p> <hr/> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>Lipman Y, Havasi M, Holderrieth P, et al. Flow Matching Guide and Code[J]. arXiv preprint arXiv:2412.06264, 2024. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:1:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p> </li> <li id="fn:2"> <p>Gat I, Remez T, Shaul N, et al. Discrete flow matching[J]. arXiv preprint arXiv:2407.15595, 2024. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:3"> <p>Coddington E A, Levinson N, Teichmann T. Theory of ordinary differential equations[J]. 1956. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="Flow_Matching"/><category term="Discrete_Flow_Matching"/><category term="formatting"/><category term="math"/><summary type="html"><![CDATA[本文基于Flow Matching Guide and Code1 ，主要是读这篇文章时候的笔记。 连续时间的马可夫链（Continuous Time Markov Chains，CTMC）是Flow的替代，也是一种生成式模型，用来生成离散的数据。它是离散型Flow Matching（Discrete Flow Matching， DFM）2的基础。 离散状态空间和随机变量 这一节就是讲一些notations。 以类比的方法可以表示离散状态空间： 连续状态空间： $\mathbb{R}^{d}$，$d$是空间维度 离散状态空间：$\mathcal{S} = \mathcal{T}^d$，$d$是空间维度，其中$\mathcal{T}=[K]={1,2,\cdots, K}$，$\mathcal{T}$也就是vocabulary。 从$\mathcal{S}$中采样的样本记为$x = (x^1,x^2,\cdots,x^d)\in\mathcal{S}$，其中$x^i$就是一个token。 用$X$表示状态空间$\mathcal{S}$中的随机变量，其pmf（probability mass function，概率质量函数，离散分布叫概率质量函数，连续分布叫概率密度函数）为\(p_X : \mathcal{S} \to \mathbb{R}_{\ge 0}\)，实际上pmf是一个从状态空间\(\mathcal{S}\)到正实数集的映射。pmf满足求和为1的约束条件：\(\sum_{x\in\mathcal{S}}p_X(x)=1\)。 对于事件$A\in\mathcal{S}$，其发生概率可以如下计算： \[\mathbb{P}(X\in A)=\sum_{x\in A}p_X(x)\] 还需要一个delta分布（好像也叫Dirac分布）的pmf表示，如下： \[\delta(x,z)=\begin{cases} +\infty \quad x=z\\ 0\quad \text{else}. \end{cases}\] 这个分布表示只有$x$取特定值$z$的时候才有概率，没有概率取其他值。但是该分布的pmf在x的定义域积分也是1。 有时候也会在token层级上定义delta分布，例如$\delta(x^i,y^i)\quad x^i,y^i \in \mathcal{T}$。 关于Dirac分布的更多信息可以参考：https://spaces.ac.cn/archives/1870 CTMC生成式模型 CTMC是连续时间的马可夫链，马可夫链是离散状态的马可夫过程。所以CTMC是连续时间离散状态的马可夫过程，可以表示为$(X_t)_{0\le t\le 1}$。因为CTMC是随机过程嘛，所以它是一族随机变量。它的马可夫假设可以表示为如下的形式： \[\mathbb{P}(X_{t+s}=z\mid X_s=x,X_u=y,0\le u\le s)=\mathbb{P}(X_{t+s}=z\mid X_s=x)\] 类比离散时间的马可夫链，这个公式说的就是只要知道$X_s$的值后，在$s$时刻之前的值（例如式子中的$X_u$），对$s$时候之后的值没有影响（式中$X_{t+s}$）。即“遗忘过去”。关于CTMC更多的介绍可以参考：https://www.math.pku.edu.cn/teachers/lidf/course/stochproc/stochprocnotes/html/_book/markovc.html#markovc-ctime。 类比扩散模型和Flow Matching，也可以定义CTMC的转移核（probability transition kernel）为$p_{t+h\mid t}$，其形式如下： \begin{equation} p_{t+h\mid t}:= \mathbb{P}(X_{t+h}=y\mid X_t=x)=\delta(y,x)+hu_t(y,x)+o(h), \quad \mathbb{P}(X_0=x)=p(x)\label{eq:transition_kernel} \end{equation} $p_{t+h\mid t}$的图示如下： CTMC模型的状态转移过程图示 公式$\eqref{eq:transition_kernel}$中，$p$表示随机过程$(X_t)_{0\le t\le 1}$在$t=0$时候的分布（取$t=0$随机过程退化成随机变量）。$o(h)$是高阶无穷小量 \[\lim_{h\to0} \cfrac{o(h)}{h} = 0\] 其中，$u_t(y,x)$叫做速率（rates或者velocities），它可以类比Flow Matching里面讲的向量场，在CTMC里面表示的是表示状态之间概率转换的速率，按我的理解，就是下面的这个形式： \begin{equation} u_t(y,x) =\lim_{h\to 0}\cfrac{\mathbb{P}(X_{t+h}=y\mid X_t=x)}{h} \label{eq:rate} \end{equation} 它是时间$t$的函数。从公式$\eqref{eq:rate}$和公式$\eqref{eq:transition_kernel}$的角度理解，$\delta(y,x)$就是发生状态转移前（即$t$时刻）的初始状态。具体讲，由于我们以$X_t=x$为条件，所以初始状态就是确定的，用概率形式表示就是$\delta(\cdot,x)$，也就是说初始状态时（即$t$时刻），在$x$处有概率，而其他状态空间中的点概率都是0。 公式$\eqref{eq:transition_kernel}$中，$u_t(y,x)$需要满足一定的条件，称为速率条件（rate conditions）： \begin{equation} u_t(y,x)\ge 0 \text{ for all } y\neq x \text{, and }\sum_{y}u_t(y,x) =0 \label{eq:rate_cond} \end{equation} 这样就是说，$u_t(x,x)&lt; 0$，且$u_t(x,x)=-\sum_{y\neq x}u_t(y,x)$。速度条件的存在是为了保证公式$\eqref{eq:transition_kernel}$算出的转移核$p_{t+h\mid t}(\cdot\mid x)\ge0$且$\sum p_{t+h\mid t}(\cdot\mid x)=1$。 类比Flow Matching，我们也可以说：如果存在$p_{t+h\mid t}$满足公式$\eqref{eq:transition_kernel}$，$p_{t+h\mid t}$的边缘分布为$p_t$，则称$u_t$生成了$p_t$。 用公式$\eqref{eq:transition_kernel}$，我们也可以用Euler法（类比扩散模型和Flow Matching里面ODE的解法）来采样，如下： \[\mathbb{P}(X_{t+h}=y\mid X_t)=\delta(y,X_t)+hu_t(y,X_t)\] 但是该式引入了一个偏差，就是$o(h)$。因为这是一个高阶小量，所以只要$h$足够小，那么$o(h)$就会趋于0，这就是希望Euler法采样准确的条件，但是这样采样步骤就会很多。一种缓解这个问题的方法是用如下的Euler法： \[\mathbb{P}(X_{t+h}=y\mid X_t) =\begin{cases} \exp \left[hu_t(X_t,X_t)\right] \qquad \qquad\qquad\qquad\qquad y=X_t \\ \cfrac{u_t(y,X_t)}{\left|u_t(X_t,X_t) \right|}\left( 1 - \exp\left[hu_t(X_t,X_t)\right] \right) \qquad y\neq X_t \end{cases}\] 概率路径和Kolmogorov方程 类比连续情况下的一致性方程（Continuity Equation），CTMC模型的边缘概率$p_t$满足Kolmogorov方程： \begin{equation} \cfrac{\mathrm{d}}{\mathrm{d}t}p_t(y)=\sum_x u_t(y,x)p_t(x) \label{eq:Kolmogorov} \end{equation} 下面这个定理描述了上式这种线性ODE系统唯一解的存在性： 定理（线性ODE的解的存在性和唯一性）：如果$u_t(y,x)$在$C([0,1))$中（即在时间$t$上连续），那么存在唯一的解$p_t(x)$满足Kolmogorov方程$\eqref{eq:Kolmogorov}$，其中，$t\in[0,1)$，初始状态$p_0(x) = p(x)$。 对CTMC而言，在$t\in[0,1)$区间，不需要额外的条件，就能够确定解一定存在。关于该定理的更多详细信息可以参考Coddington et al. (1956)3中的定理5.1和5.2。 方程$\eqref{eq:Kolmogorov}$可以化为： \[\begin{equation} \begin{aligned} \sum_x u_t(y,x)p_t(x) &amp;=\underbrace{\sum_{x\neq y} u_t(y,x)p_t(x)}_{输入通量}-\underbrace{\sum_{x\neq y}u_t(x,y)p_t(y)}_{输出通量} \\ &amp;=-\sum_{x\neq y}\left[j_t(x,y)-j_t(y,x)\right] \\ \end{aligned} \end{equation}\] 其中，$j_t(y,x):=u_t(y,x)p_t(x)$，称为概率通量（probability flux）。上式很好理解，就是把从$x$输入到$y$的通量分成了从$x$输入到$y$的部分和从$y$输出到$x$的部分。 将输出通量超出来的部分定义为散度，即 \[\text{div}(u_t p_t)(y) =\sum_{x\neq y}\left[j_t(x,y)-j_t(y,x)\right]\] 那么Kolmogorov方程就可以表示为： \[\dot p_t(y)+\text{div}(u_t p_t)(y)=0\] 这和连续型Flow Matching的一致性方程统一起来了。 以下还有一个结果，是在CTMC 框架中构建概率路径和速度的主要工具： 定理 （离散质量守恒，Discrete Mass Conservation）：$u_t(y,x)$在$C([0,1))$中，$p_t(x)$在$C^1([0,1))$中（我的理解是一阶导数在时间$t$上连续），则如下两个结论是等价的： 在$t\in[0,1)$时， $p_t,u_t$满足Kolmogorov方程$\eqref{eq:Kolmogorov}$，$u_t$满足速率条件$\eqref{eq:rate_cond}$。 在$t\in[0,1)$时，$u_t$生成（用前文的术语）了$p_t$。 该定理的证明在1的appendix A.1.。 保概率速率（Probability preserving velocities） 如果$u_t(y,x)$生成了概率路径$p_t(x)$，我们构造一个新的速率： \[\tilde u_t(y,x) = u_t(y,x) + v_t(y,x)\] 只要$v_t(y,x)$满足速率条件且 \[\sum_x v_t(y,x)p_t(x) = 0\] 那么$\tilde u_t(y,x)$还能Kolmogorov方程： \[\sum_x \tilde u_t(y,x)p_t(x) = \sum_x u_t(y,x)p_t(x) = \dot p_t(y)\] 那么$\tilde u_t(y,x)$还能生成$p_t(x)$。这个结论在离散型Flow Matching里面非常重要。 Lipman Y, Havasi M, Holderrieth P, et al. Flow Matching Guide and Code[J]. arXiv preprint arXiv:2412.06264, 2024. &#8617; &#8617;2 Gat I, Remez T, Shaul N, et al. Discrete flow matching[J]. arXiv preprint arXiv:2407.15595, 2024. &#8617; Coddington E A, Levinson N, Teichmann T. Theory of ordinary differential equations[J]. 1956. &#8617;]]></summary></entry></feed>