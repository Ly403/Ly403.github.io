<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> 离散型流匹配模型（3） | Yi Liu's Homepage. 😀 </title> <meta name="author" content="Yi LIU"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?b05f9a0b7405d7c8c89c7465593dea81"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?dadeb9c5d1fd12bc8d37475657446863"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?1fefb6021bf36010f25ea1cef24af84e"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?53a094b51ed1d1e025731eb00d240058" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%91%8B&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ly403.github.io/blog/2025/%E7%A6%BB%E6%95%A3%E6%B5%81%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-3/"> <script src="/assets/js/theme.js?f2531f05c6f8e1622518f4f5a1e385b1"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?46af317e693b09921dcb92261d123fbc" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Yi Liu's Homepage. 😀 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/teaching/">teaching</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">离散型流匹配模型（3）</h1> <p class="post-meta"> Created on January 20, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/formatting"> <i class="fa-solid fa-hashtag fa-sm"></i> formatting</a>   <a href="/blog/tag/math"> <i class="fa-solid fa-hashtag fa-sm"></i> math</a>   ·   <a href="/blog/category/flow-matching"> <i class="fa-solid fa-tag fa-sm"></i> Flow_Matching</a>   <a href="/blog/category/discrete-flow-matching"> <i class="fa-solid fa-tag fa-sm"></i> Discrete_Flow_Matching</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>接着上次说的DFM的分解技巧，继续学习DFM的最后一部分——混合路径（Mixture paths）。</p> <h2 id="混合路径mixture-paths">混合路径（Mixture paths）</h2> <p>我们在此前的文章里面已经讲解了分解技巧<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>，得到了分解的（边缘/条件）概率路径、分解的（边缘/条件）速率，以及知道了他们之间的关系（<sup id="fnref:1:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>中的命题2和定理16）。现在我们要着手设计出一个具体的DFM了，类似FM<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>一样，<strong>我们可以从分解条件概率路径着手，然后得到目标分解条件速率，使用神经网络预测分解条件速率，最后使用分解CDFM损失（<sup id="fnref:1:2"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>中公式14）来训练模型。</strong></p> <p>我们以$Z=(X_0,X_1) \sim \pi_{0,1}(X_0,X_1)$为条件（$X_0$和$X_1$为任意配对），那么分解的条件概率路径可以使用如下形式：</p> \[p_{t\mid 0,1}^i (x^i\mid x_0,x_1)=\kappa_t\delta(x^i,x_1^i) + (1-\kappa_t)\delta(x^i,x_0^i) \tag{1}\] <p>这个做法类似Rectified Flow（<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>中公式1），只是使用了更general的$\kappa_t$作为系数，$\kappa : [0,1] \to [0,1]$就类似扩散模型里面的噪声调度器，$\kappa_t \in C^1([0,1])$。根据<sup id="fnref:1:3"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>中公式6，我们能得到非分解的条件概率路径：</p> \[p_{t\mid 0,1} (x\mid x_0,x_1)=\prod_i p_{t\mid 0,1}^i (x^i\mid x_0,x_1) \tag{2}\] <blockquote> <p>关于公式1的详细解释：</p> <p>分解概率路径上的随机变量$X_t^i \sim p_{t\mid 0,1}^i(\cdot\mid x_0,x_1) $满足：</p> \[X_t^i = \begin{cases} x_1^i \quad 概率为\kappa_t\\ x_0^i \quad 概率为(1-\kappa_t) \end{cases} \tag{3}\] <p>也就是对$t$时刻的第$i$个token $X_t^i$，它要么取源状态的值$x_0^i$，要么取目标状态的值$x_1^i$，取这两个值的概率与时间$t$相关。</p> </blockquote> <p>从公式1和2给出的条件概率路径可以算出边缘概率路径，即：</p> \[p_t(x)=\sum_{x_0,x_1}p_{t\mid 0,1}(x\mid x_0,x_1) p_{0,1}(x_0,x_1) \tag{4}\] <p>需要指出的是，<strong>$p_t(x)$需要满足边缘条件，即$p_0(x) = \delta(x,x_0)$，$p_1(x) = \delta(x,x_1)$。为实现这个条件，需要让$\kappa_0 = 0$，$\kappa_1 = 1$。</strong></p> <p>接下来，我们需要知道公式1所示分解条件概率路径对应的分解条件速率$u_t^i(y^i,x^i\mid x_0,x_1)$。由Kolmogorov方程可知：</p> \[\cfrac{\mathrm{d}}{\mathrm{d}t}p_{t\mid 0,1}^i(y^i\mid x_0,x_1) = \sum_{x^i}u_t^i(y^i,x^i\mid x_0,x_1)p_{t\mid 0,1}^i(x^i\mid x_0,x_1) \tag{5}\] <p>结合公式1给出的具体分解条件概率路径，可以算出：</p> \[\begin{aligned} \cfrac{\mathrm{d}}{\mathrm{d}t}p_{t\mid 0,1}^i(y^i\mid x_0,x_1)&amp; \overset{公式1}{=} \dot \kappa_t\left[\delta(y^i,x_1^i) - \delta(y^i,x_0^i)\right] \\ &amp;\overset{用公式1消去\delta(y^i,x_0^i)}{=} \dot \kappa_t\left[\delta(y^i,x_1^i) - \cfrac{p^i_{t\mid 0,1}(y^i\mid x_0,x_1)-\kappa_t\delta(y^i,x_1^i)}{1-\kappa_t}\right]\\ &amp;{=} \cfrac{\dot \kappa_t}{1 - \kappa_t}\left[({1-\kappa_t})\delta(y^i,x_1^i) - {p^i_{t\mid 0,1}(y^i\mid x_0,x_1)+\kappa_t\delta(y^i,x_1^i)}\right]\\ &amp;{=} \cfrac{\dot \kappa_t}{1 - \kappa_t} \left[ \delta(y^i,x_1^i) - p^i_{t\mid 0,1}(y^i\mid x_0,x_1) \right]\\ &amp;\overset{(*)}{=}\sum_{x^i} \cfrac{\dot \kappa_t}{1 - \kappa_t} \left[ \delta(y^i,x_1^i) - \delta(y^i,x^i) \right] p^i_{t\mid 0,1}(x^i\mid x_0,x_1)\\ \end{aligned}\tag{6}\] <blockquote> <p>关于（*）步的解释如下：</p> <p>这里其实就是要证明</p> \[\delta(y^i,x_1^i) - p^i_{t\mid 0,1}(y^i\mid x_0,x_1) = \sum_{x^i} \left[ \delta(y^i,x_1^i) - \delta(y^i,x^i) \right] p^i_{t\mid 0,1}(x^i\mid x_0,x_1) \tag{7}\] <p>我们从右边开始变换：</p> \[\begin{aligned} \sum_{x^i} \left[ \delta(y^i,x_1^i) - \delta(y^i,x^i) \right] p^i_{t\mid 0,1}(x^i\mid x_0,x_1) &amp;= \sum_{x^i} \left[ \delta(y^i,x_1^i)p^i_{t\mid 0,1}(x^i\mid x_0,x_1) - \delta(y^i,x^i)p^i_{t\mid 0,1}(x^i\mid x_0,x_1) \right]\\ &amp;= \sum_{x^i} \delta(y^i,x_1^i)p^i_{t\mid 0,1}(x^i\mid x_0,x_1) - \sum_{x^i} \delta(y^i,x^i)p^i_{t\mid 0,1}(x^i\mid x_0,x_1) \\ &amp;\overset{(1)}{=} \sum_{x^i} \delta(y^i,x_1^i)p^i_{t\mid 0,1}(x^i\mid x_0,x_1) - p^i_{t\mid 0,1}(y^i\mid x_0,x_1) \\ &amp;\overset{(2)}{=} \delta(y^i,x_1^i) - p^i_{t\mid 0,1}(y^i\mid x_0,x_1) \\ \end{aligned} \tag{8}\] <p>第（1）步是delta函数的性质，$\int_x\delta(y,x)f(x)\mathrm{d}x = f(y)$，第（2）步是因为$\delta(y^i,x_1^i)$与$x^i$无关，$\sum_{x^i} p^i_{t\mid 0,1}(x^i\mid x_0,x_1) = 1 $。</p> </blockquote> <p>对比公式6的最后结果和公式5，就知道：</p> \[u_t^i(y^i,x^i\mid x_0,x_1) = \cfrac{\dot \kappa_t}{1 - \kappa_t} \left[ \delta(y^i,x_1^i) - \delta(y^i,x^i) \right] \tag{9}\] <p>这样我们就知道了公式1所示的分解条件概率路径对应的分解条件速率$u_t^i(y^i,x^i\mid x_0,x_1)$的具体形式了。</p> <p>Code 9是混合路径的实现代码。</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post/DFM31-480.webp 480w,/assets/img/post/DFM31-800.webp 800w,/assets/img/post/DFM31-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post/DFM31.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> 混合路径的实现代码 </div> <h3 id="速率后验参数化">速率后验参数化</h3> <p>类比我们在FM<sup id="fnref:2:1"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>和扩散模型里面也会讲的多种预测方式，例如velocity-prediction、$x_0$-prediction、$x_1$-prediction，我们在DFM里面也可以使用类似的预测方式。</p> <p>最简单的一种：<strong>使用神经网络预测公式9给出的分解条件速率$u_t^i(y^i,x^i\mid x_0,x_1)$，这就是velocity-prediction。</strong></p> <p>接下来介绍DFM里面的$x_1$-prediction，这个会稍微复杂一点。</p> <p>根据我们在<sup id="fnref:1:4"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>中证明出来的公式12，可以得到在$u_t^i(y^i,x^i\mid x_0,x_1)$取公式9的形式时，分解边缘速率为：</p> \[\begin{aligned} u_t^i(y^i,x) &amp;=\sum_{x_0,x_1} u_t^i(y^i,x^i\mid x_0,x_1)p_{0,1\mid t}(x_0,x_1\mid x)\\ &amp;\overset{公式(9)}{=} \sum_{x_0,x_1} \cfrac{\dot \kappa_t}{1 - \kappa_t} \left[ \delta(y^i,x_1^i) - \delta(y^i,x^i) \right] p_{0,1\mid t}(x_0,x_1\mid x)\\ &amp;\overset{将x_1^i分出来}{=} \sum_{x_1^i}\sum_{x_0,x_1^{\bar i}} \cfrac{\dot \kappa_t}{1 - \kappa_t} \left[ \delta(y^i,x_1^i) - \delta(y^i,x^i) \right] p_{0,1\mid t}(x_0,x_1\mid x)\\ &amp;\overset{分配律}{=} \sum_{x_1^i} \cfrac{\dot \kappa_t}{1 - \kappa_t} \left[ \delta(y^i,x_1^i) - \delta(y^i,x^i) \right]\sum_{x_0,x_1^{\bar i}} p_{0,1\mid t}(x_0,x_1\mid x)\\ \end{aligned} \tag{10}\] <p>单独把第二个求和式拿出来，记为：</p> \[p_{1\mid t}^i(x_1^i\mid x) = \sum_{x_0,x_1^{\bar i}} p_{0,1\mid t}(x_0,x_1\mid x) \overset{(*)}{=}\mathbb{E}\left[ \delta(x_1^i,X_1^i) \mid X_t = x \right] \tag{11}\] <blockquote> <p>关于公式11中（*）步骤的详细说明：</p> \[\begin{aligned} p_{1\mid t}^i(x_1^i\mid x) &amp;= \sum_{x_0,x_1^{\bar i}} p_{0,1\mid t}(x_0,x_1\mid x)\\ &amp;\overset{拆开x_1}{=} \sum_{x_0,x_1^{\bar i}} p_{0,1\mid t}(x_0,x_1^{\bar i},x_1^i\mid x)\\ &amp;\overset{\delta函数的性质}{=} \sum_{x_0,x_1^{\bar i}}\sum_{X_1^i}\delta(x_1^i,X_1^i) p_{0,1\mid t}(x_0,x_1^{\bar i},X_1^i\mid x)\\ &amp;=\mathbb{E}\left[ \delta(x_1^i,X_1^i) \mid X_t = x \right] \end{aligned} \tag{12}\] </blockquote> <p>我们可以用神经网络去学习后验分布$p_{1\mid t}^{\theta,i}(x_1^i\mid x)$，$\theta$为神经网络的参数。这就是离散版的$x_1$-prediction。</p> <p>这样理解：以文本为例，<strong>$x$是时间$t$时的“加噪”文本（但其实说“加噪“不准确，因为DFM的初始状态$x_0$是一个全mask的文本，$x$更类似一个处于中间状态的文本）。$x$作为神经网络的输入，神经网络预测$x_1^i$，$i\in [d]$表示这是$x_1$的第$i$个token，$d$为句长。神经网络一次预测所有token的单步变化概率，每个token共有$K$种变化情况（$K$为vocabulary的大小），神经网络的输出向量的维度就是$d\cdot K$。</strong></p> <h3 id="混合路径的cdfm损失">混合路径的CDFM损失</h3> <p>我们基于$x_1$-prediction，即预测$p_{1\mid t}^{\theta,i}(x_1^i\mid x)$，继续讨论公式1所示混合路径情况下的CDFM损失。这里会有两种做法。</p> <p>结合公式11，如果直接对比后验概率，则CDFM损失可以写为：</p> \[L_{CDFM}(\theta) = \mathbb{E}_{t,X_0,X_1,X_t} D_{X_t}\left( \delta(\cdot,X_1^i),p_{1\mid t}^{\theta,i}(\cdot\mid X_t) \right) \tag{13}\] <p>$\delta(\cdot,X_1^i)$，$p_{1\mid t}^{\theta,i}(\cdot\mid X_t)$都是概率质量函数，所以可以使用KL散度衡量二者之间的距离，即取Bregman散度为KL散度，得到：</p> \[L_{CDFM}(\theta) = - \mathbb{E}_{t,X_0,X_1,X_t} \log p_{1\mid t}^{\theta,i}(X_1^i\mid X_t) + \text{const} \tag{14}\] <p>这是第一种做法。</p> <blockquote> <p>关于公式14的详细说明：</p> <p>KL散度的公式是：$D(p,q)=\sum_{\alpha \in \mathcal{T}}p(\alpha)\log\cfrac{p(\alpha)}{q(\alpha)}$。在选择KL散度为Bregman散度后，公式13可以化为：</p> \[\begin{aligned} L_{CDFM}(\theta) &amp;= \mathbb{E}_{t,X_0,X_1,X_t} \sum_{X_1^i} \delta(x_1^i,X_1^i) \log\cfrac{\delta(x_1^i,X_1^i)}{p_{1\mid t}^{\theta,i}( X_1^i\mid X_t)}\\ &amp;=\mathbb{E}_{t,X_0,X_1,X_t} \sum_{X_1^i}\left[ \delta(x_1^i,X_1^i) \log{\delta(x_1^i,X_1^i)} - \delta(x_1^i,X_1^i) \log{p_{1\mid t}^{\theta,i}( X_1^i\mid X_t)}\right]\\ &amp;= \text{const} - \mathbb{E}_{t,X_0,X_1,X_t} \sum_{X_1^i}\delta(x_1^i,X_1^i) \log{p_{1\mid t}^{\theta,i}( X_1^i\mid X_t)}\\ &amp;\overset{\delta函数的性质}{=} \text{const} - \mathbb{E}_{t,X_0,X_1,X_t} \log{p_{1\mid t}^{\theta,i}(x_1^i\mid X_t)} \end{aligned} \tag{15}\] </blockquote> <p>第二种做法是使用<sup id="fnref:1:5"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>中公式14所示的分解CDFM损失，如下：</p> \[L_{CDFM}(\theta) = \mathbb{E}_{t,Z,X_t\sim p_{t\mid Z}} \sum_i D_{X_t}^i \left( u_t^i(\cdot,X_t \mid Z), u_t^{\theta,i}(\cdot, X_t) \right) \tag{16}\] <p>取$Z=(X_0,X_1)$。使用该公式需要知道$u_t^{\theta,i}$，因为神经网络预测了$p_{1\mid t}^{\theta,i}$，所以$u_t^{\theta,i}$是能算出来的。</p> <p>公式16中的Bregman散度可以选择广义的KL损失（其输入不一定要是概率分布）。具体而言，对于向量$u,v\in \mathbb R_{\ge 0}^{m}$，广义KL损失是：</p> \[D(u,v) = \sum_j u^j \log \cfrac{u^j}{v^j} - \sum_j u_j + \sum_jv_j \tag{17}\] <p>这种选择下对应的Bregman散度为：</p> \[D\left(u_t^i(\cdot,x^i\mid x_0,x_1),u^{i,\theta}_t(\cdot,x)\right)= \cfrac{\dot \kappa_t}{1-\kappa_t}\left[ \left(\delta(x_1^i,x^i) - 1\right)\log p_{1\mid t}^{i,\theta}(x_1^i\mid x) + \delta(x_1^i,x^i)- p_{1\mid t}^{i,\theta}(x^i\mid x) \right] \tag{18}\] <h3 id="混合轨迹采样">混合轨迹采样</h3> <p>我们在<sup id="fnref:1:6"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>中的公式5已经给出per coordinate的采样方式，结合前面讨论的混合路径，采样公式可以写为：</p> \[\mathbb{P}(X^i_{t+h}=y^i\mid X_t = x) =\delta(y^i,x^i) + h u_t^i(y^i,x) +o(h) \tag{19}\] <p>在混合路径的特定情况下为：</p> \[\begin{aligned} \mathbb{P}(X^i_{t+h}=y^i\mid X_t = x) &amp;\overset{公式10}{=} \delta(y^i,x^i) + o(h) + h \sum_{x_1^i} \cfrac{\dot \kappa_t}{1 - \kappa_t} \left[ \delta(y^i,x_1^i) - \delta(y^i,x^i) \right]\sum_{x_0,x_1^{\bar i}} p_{0,1\mid t}(x_0,x_1\mid x)\\ &amp;\overset{公式11}{=} \delta(y^i,x^i) + o(h) + h \sum_{x_1^i} \cfrac{\dot \kappa_t}{1 - \kappa_t} \left[ \delta(y^i,x_1^i) - \delta(y^i,x^i) \right] p_{1\mid t}^i(x_1^i\mid x)\\ &amp;\overset{\sum_{x_1^i}p_{1\mid t}^i(x_1^i\mid x)=1 }{=} \sum_{x_1^i}\left[ \delta(y^i,x^i) + h \cfrac{\dot \kappa_t}{1 - \kappa_t} \left[ \delta(y^i,x_1^i) - \delta(y^i,x^i) \right] + o(h) \right] p_{1\mid t}^i(x_1^i\mid x)\\ \end{aligned} \tag{20}\] <p>在已知$X_t = x$的情况下，使用公式20进行采样的具体步骤如下：</p> <ol> <li>从$p_{1\mid t}^i(X_1^i\mid x)$采样出$X_1^i$，$X_1^i\sim p_{1\mid t}^i(X_1^i\mid x)$。$p_{1\mid t}^i(X_1^i\mid x)$是由模型预测得到。</li> <li>基于$X_t^i$更新出$X_{t+h}^i$。这一步需要使用<sup id="fnref:5"><a href="#fn:5" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>中公式9所示的Euler step，并将其中的速率设置为$\cfrac{\dot \kappa_t}{1 -\kappa_t}\left[\delta(y^i,X_1^i) - \delta(y^i,x^i)\right]$。<strong>这一步决定了$X_{t+h}^i=X_t^i$还是$X_{t+h}^i=X_1^i$</strong>。</li> </ol> <h3 id="单边混合路径和保概率速率">单边混合路径和保概率速率</h3> <p>我们在第一次讲CTMC的那篇文章<sup id="fnref:5:1"><a href="#fn:5" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>里面的最后一部分讲了一下保概率速率，到这里终于要用到了。</p> <p>简单回顾一下在CTMC模型里面讲到的保概率速率，它简单的说就是<strong>在已有的速率$u_t(y,x)$中加入另一个速率$v_t(y,x)$，只要$v_t(y,x)$是divergence-free的，那么$u_t(y,x)$生成概率路径不会改变</strong>。所谓divergence-free，指$\sum_x v_t(y,x)p_t(x) = 0$。</p> <p>对DFM里面的分解条件概率路径$p_{t\mid Z}^i(x^i\mid z)$，其对应的保概率条件速率$v_t^i(y^i,x^i\mid z)$满足：</p> \[\sum_{x^i} v_t^i(y^i,x^i\mid z)p_{t\mid Z}^i(x^i\mid z) = 0 \tag{21}\] <p>一般来说这个保概率条件速率$v_t^i(y^i,x^i\mid z)$不是特别好找到。但是如果我们能有如下两个假设：</p> <ol> <li>iid的目标分布： $p(x) = \prod_i p(x^i)$。</li> <li>源分布和目标分布的数据独立配对：$\pi_{0,1}(x_0,x_1) = p(x_0)q(x_1)$。</li> </ol> <p>那么保概率条件速率的解析式还是比较好找到的。接下来我们来构造这个保概率条件速率。</p> <p>根据前面的两条假设，能写出混合路径的形式如下：</p> \[p_t(x) = \sum_{x_1} p_{t\mid 1}(x\mid x_1)q(x_1) \text{ where } p_{t\mid 1} (x) = \prod_i p_{t\mid 1}^i(x^i \mid x_1)\\ p_{t\mid 1}^i(x^i \mid x_1) = \kappa_t \delta(x^i,x_1^i) + (1 - \kappa_t)p(x^i) \tag{22}\] <p>这个写法和我们前面的混合概率路径的写法的差异就是只用$x_1$作为条件，但其实它和用$x_0,x_1$都作为条件应该是等价的。</p> <p>类比公式6</p> \[\begin{aligned} \cfrac{\mathrm{d}}{\mathrm{d}t}p_{t\mid 1}^i(y^i\mid x_1) &amp; \overset{公式22}{=} \dot \kappa_t\left[\delta(y^i,x_1^i) - p(x^i)\right] \\ &amp;\overset{用公式22消去p(x^i)}{=} \dot \kappa_t\left[\delta(y^i,x_1^i) - \cfrac{p^i_{t\mid 1}(y^i\mid x_1)-\kappa_t\delta(y^i,x_1^i)}{1-\kappa_t}\right]\\ &amp;{=} \cfrac{\dot \kappa_t}{1 - \kappa_t}\left[({1-\kappa_t})\delta(y^i,x_1^i) - {p^i_{t\mid 1}(y^i\mid x_1)+\kappa_t\delta(y^i,x_1^i)}\right]\\ &amp;{=} \cfrac{\dot \kappa_t}{1 - \kappa_t} \left[ \delta(y^i,x_1^i) - p^i_{t\mid 1}(y^i\mid x_1) \right]\\ &amp;{=}\sum_{x^i} \cfrac{\dot \kappa_t}{1 - \kappa_t} \left[ \delta(y^i,x_1^i) - \delta(y^i,x^i) \right] p^i_{t\mid 1}(x^i\mid x_1)\\ \end{aligned}\tag{23}\] <p>对比Kolmogorov方程，知道：</p> \[u_t^i(y^i,x^i\mid x_1)=\cfrac{\dot \kappa_t}{1 - \kappa_t} \left[ \delta(y^i,x_1^i) - \delta(y^i,x^i) \right] \tag{24}\] <p>可见$u_t^i(y^i,x^i\mid x_1)$生成$p^i_{t\mid 1}(x^i\mid x_1)$。如果要找到对$p^i_{t\mid 1}(x^i\mid x_1)$而言divergence-free的条件速率，一个简单的技巧是<strong>找到一个和$u_t^i(y^i,x^i\mid x_1)$方向相反的条件速率<sup id="fnref:6"><a href="#fn:6" class="footnote" rel="footnote" role="doc-noteref">5</a></sup>，这样在总时间范围内折两个速率差的散度和就是0，所以这两速率的差就是我们要找的divergence-free的条件速率，也就是保概率条件速率。</strong>记这个反向的条件速率为$\tilde u_t^i(y^i,x^i\mid x_1)$，它的形式应该是：</p> \[\tilde u_t^i(y^i,x^i\mid x_1) = \cfrac{\dot \kappa_t}{\kappa_t} \left[ \delta(y^i,x^i) - p(x^i) \right] \tag{25}\] <p>证明在<sup id="fnref:6:1"><a href="#fn:6" class="footnote" rel="footnote" role="doc-noteref">5</a></sup>的E.5和E.6中。</p> <p>所以divergence-free的速率是：</p> \[v_t^i(y^i,x^i \mid x_1) =u_t^i(y^i,x^i\mid x_1) -\tilde u_t^i(y^i,x^i\mid x_1)\tag{26}\] <p><strong>因为$v_t^i(y^i,x^i \mid x_1)$是divergence-free的，所以把它以任意比例加到原来的速率里面都不会影响生成的概率路径</strong>。所以公式10所示分解边缘速率如果改写成下面的形式：</p> \[\begin{aligned} u_t^i(y^i,x) &amp;=\sum_{x_1} \left[ u_t^i(y^i,x^i\mid x_1) + c_t v_t^i(y^i,x^i\mid x_1) \right]p_{1\mid t}(x_1 \mid x)\\ &amp;=\sum_{x_1^i} \left[ u_t^i(y^i,x^i\mid x_1^i) + c_t v_t^i(y^i,x^i\mid x_1^i) \right]p^i_{1\mid t}(x^i_1 \mid x)\\ \end{aligned} \tag{27}\\\] <p>不影响所生成的边缘概率路径。第二个等式是因为$u_t^i(y^i,x^i\mid x_1) =u_t^i(y^i,x^i\mid x_1^i)$，$v_t^i(y^i,x^i\mid x_1) =v_t^i(y^i,x^i\mid x_1^i)$。</p> <p>那么，用公式27所示的速率进行采样的步骤是：</p> <ol> <li> <p>从$p_{1\mid t}^i(X_1^i\mid x)$采样出$X_1^i$，$X_1^i\sim p_{1\mid t}^i(X_1^i\mid x)$。$p_{1\mid t}^i(X_1^i\mid x)$是由模型预测得到。</p> </li> <li> <p>基于$X_t^i$更新出$X_{t+h}^i$。这一步需要使用<sup id="fnref:5:2"><a href="#fn:5" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>中公式9所示的Euler step，并将其中的速率设置为</p> \[u_t^i(y^i,x^i\mid x_1) = \cfrac{\dot \kappa_t}{1-\kappa_t}\left[\delta(y^i,X_1^i) - \delta(y^i,x^i)\right] +c_t\left[ \cfrac{\dot \kappa_t}{1-\kappa_t}\left[\delta(y^i,x_1^i) - \delta(y^i,x^i)\right] - \cfrac{\dot \kappa_t}{\kappa_t} \left[\delta(y^i,x^i) - p(x^i)\right] \right] \tag{28}\] <p>其中$c_t&gt;0$是与时间相关的常数。</p> </li> </ol> <p>在<a href="https://github.com/facebookresearch/flow_matching" rel="external nofollow noopener" target="_blank">https://github.com/facebookresearch/flow_matching</a>库里面实现的DFM的代码如下：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>  

<span class="kn">from</span> <span class="n">flow_matching.path</span> <span class="kn">import</span> <span class="n">MixtureDiscreteProbPath</span><span class="p">,</span> <span class="n">DiscretePathSample</span>  
<span class="kn">from</span> <span class="n">flow_matching.path.scheduler</span> <span class="kn">import</span> <span class="n">PolynomialConvexScheduler</span>  
<span class="kn">from</span> <span class="n">flow_matching.loss</span> <span class="kn">import</span> <span class="n">MixturePathGeneralizedKL</span>  
<span class="kn">from</span> <span class="n">flow_matching.solver</span> <span class="kn">import</span> <span class="n">MixtureDiscreteEulerSolver</span>  
<span class="kn">from</span> <span class="n">flow_matching.utils</span> <span class="kn">import</span> <span class="n">ModelWrapper</span>  

<span class="c1"># Define a trainable velocity model  
</span><span class="n">model</span> <span class="o">=</span> <span class="p">...</span>   

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span>  

<span class="n">scheduler</span> <span class="o">=</span> <span class="nc">PolynomialConvexScheduler</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>  
<span class="n">path</span> <span class="o">=</span> <span class="nc">MixtureDiscreteProbPath</span><span class="p">(</span><span class="n">scheduler</span><span class="o">=</span><span class="n">scheduler</span><span class="p">)</span>  
<span class="n">loss_fn</span> <span class="o">=</span> <span class="nc">MixturePathGeneralizedKL</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="n">path</span><span class="p">)</span>  <span class="c1"># Generalized KL Bregman divergence  
</span>
<span class="k">for</span> <span class="n">x_0</span><span class="p">,</span> <span class="n">x_1</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>  <span class="c1"># Samples from π0,1 of shape [batch_size, *data_dim]  
</span>    <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="mf">1e-3</span><span class="p">)</span>  <span class="c1"># Randomize time t ∼ U [0, 1 − 10−3]  
</span>    <span class="n">sample</span><span class="p">:</span> <span class="n">DiscretePathSample</span> <span class="o">=</span> <span class="n">path</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="n">t</span><span class="p">,</span> <span class="n">x_0</span><span class="o">=</span><span class="n">x_0</span><span class="p">,</span> <span class="n">x_1</span><span class="o">=</span><span class="n">x_1</span><span class="p">)</span>  <span class="c1"># Sample the conditional path  
</span>    <span class="n">model_output</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">sample</span><span class="p">.</span><span class="n">x_t</span><span class="p">,</span> <span class="n">sample</span><span class="p">.</span><span class="n">t</span><span class="p">)</span>  

    <span class="n">loss</span> <span class="o">=</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">model_output</span><span class="p">,</span> <span class="n">x_1</span><span class="o">=</span><span class="n">sample</span><span class="p">.</span><span class="n">x_1</span><span class="p">,</span> <span class="n">x_t</span><span class="o">=</span><span class="n">sample</span><span class="p">.</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="n">sample</span><span class="p">.</span><span class="n">t</span><span class="p">)</span>  <span class="c1"># CDFM loss  
</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>  
    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>  
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>  

<span class="k">class</span> <span class="nc">ProbabilityDenoiser</span><span class="p">(</span><span class="n">ModelWrapper</span><span class="p">):</span>  
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">**</span><span class="n">extras</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>  
        <span class="n">logits</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="o">**</span><span class="n">extras</span><span class="p">)</span>  
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">.</span><span class="nf">float</span><span class="p">(),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  

<span class="c1"># Sample X1  
</span><span class="n">probability_denoiser</span> <span class="o">=</span> <span class="nc">ProbabilityDenoiser</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>  
<span class="n">x_0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">*</span><span class="n">data_dim</span><span class="p">])</span>  <span class="c1"># Specify the initial condition  
</span><span class="n">solver</span> <span class="o">=</span> <span class="nc">MixtureDiscreteEulerSolver</span><span class="p">(</span>  
    <span class="n">model</span><span class="o">=</span><span class="n">probability_denoiser</span><span class="p">,</span>  
    <span class="n">path</span><span class="o">=</span><span class="n">path</span><span class="p">,</span>  
    <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">vocabulary_size</span>  
<span class="p">)</span>  

<span class="n">step_size</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">100</span>  
<span class="n">x_1</span> <span class="o">=</span> <span class="n">solver</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">x_init</span><span class="o">=</span><span class="n">x_0</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="n">step_size</span><span class="p">,</span> <span class="n">time_grid</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="mf">1e-3</span><span class="p">]))</span>
</code></pre></div></div> <p>一个独立的DFM的代码案例如下：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>  
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>  
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">Tensor</span>  
<span class="kn">from</span> <span class="n">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_moons</span>  

<span class="k">class</span> <span class="nc">DiscreteFlow</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>  
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">h</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span> <span class="n">v</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">):</span>  
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>  
        <span class="n">self</span><span class="p">.</span><span class="n">v</span> <span class="o">=</span> <span class="n">v</span>  
        <span class="n">self</span><span class="p">.</span><span class="n">embed</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span>  
        <span class="n">self</span><span class="p">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>  
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">dim</span> <span class="o">*</span> <span class="n">h</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ELU</span><span class="p">(),</span>  
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ELU</span><span class="p">(),</span>  
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ELU</span><span class="p">(),</span>  
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">dim</span> <span class="o">*</span> <span class="n">v</span><span class="p">)</span>  
        <span class="p">)</span>  

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x_t</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>  
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">net</span><span class="p">(</span>  
            <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">(</span>  
                <span class="p">(</span><span class="n">t</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">],</span> <span class="n">self</span><span class="p">.</span><span class="nf">embed</span><span class="p">(</span><span class="n">x_t</span><span class="p">).</span><span class="nf">flatten</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span> <span class="o">-</span><span class="mi">1</span>  
            <span class="p">)</span>  
        <span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">x_t</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">v</span><span class="p">])</span>  

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">256</span>  
<span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">128</span>  

<span class="n">model</span> <span class="o">=</span> <span class="nc">DiscreteFlow</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">)</span>  
<span class="n">optim</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>  

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>  
    <span class="n">x_1</span> <span class="o">=</span> <span class="nc">Tensor</span><span class="p">(</span><span class="nf">make_moons</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>  
    <span class="n">x_1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="n">x_1</span> <span class="o">*</span> <span class="mi">35</span> <span class="o">+</span> <span class="mi">50</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="n">vocab_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)).</span><span class="nf">long</span><span class="p">()</span>  
    <span class="n">x_0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>  

    <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>  
    <span class="n">x_t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">t</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">],</span> <span class="n">x_1</span><span class="p">,</span> <span class="n">x_0</span><span class="p">)</span>  

    <span class="n">logits</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>  
    <span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">.</span><span class="nf">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">x_1</span><span class="p">.</span><span class="nf">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)).</span><span class="nf">mean</span><span class="p">()</span>  
    <span class="n">optim</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>  
    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>  
    <span class="n">optim</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>  

<span class="n">x_t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>  
<span class="n">t</span> <span class="o">=</span> <span class="mf">0.0</span>  
<span class="n">results</span> <span class="o">=</span> <span class="p">[(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">)]</span>  
<span class="k">while</span> <span class="n">t</span> <span class="o">&lt;</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="mf">1e-3</span><span class="p">:</span>  
    <span class="n">p1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="nf">model</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span> <span class="o">*</span> <span class="n">t</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  
    <span class="n">h</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">t</span><span class="p">)</span>  
    <span class="n">one_hot_x_t</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">one_hot</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span>  
    <span class="n">u</span> <span class="o">=</span> <span class="p">(</span><span class="n">p1</span> <span class="o">-</span> <span class="n">one_hot_x_t</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">t</span><span class="p">)</span>  
    <span class="n">x_t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">distributions</span><span class="p">.</span><span class="nc">Categorical</span><span class="p">(</span><span class="n">probs</span><span class="o">=</span><span class="n">one_hot_x_t</span> <span class="o">+</span> <span class="n">h</span> <span class="o">*</span> <span class="n">u</span><span class="p">).</span><span class="nf">sample</span><span class="p">()</span>  
    <span class="n">t</span> <span class="o">+=</span> <span class="n">h</span>  
    <span class="n">results</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">))</span>  

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">results</span><span class="p">),</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  
<span class="nf">for </span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">),</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">axes</span><span class="p">):</span>  
    <span class="n">ax</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x_t</span><span class="p">.</span><span class="nf">detach</span><span class="p">()[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x_t</span><span class="p">.</span><span class="nf">detach</span><span class="p">()[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>  
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">t=</span><span class="si">{</span><span class="n">t</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>  
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>  
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <p>DFM生成效果如下：</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post/DFM33-480.webp 480w,/assets/img/post/DFM33-800.webp 800w,/assets/img/post/DFM33-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post/DFM33.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> DFM生成效果 </div> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p><a href="https://zhuanlan.zhihu.com/p/18450992825" rel="external nofollow noopener" target="_blank">https://zhuanlan.zhihu.com/p/18450992825</a> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">↩</a> <a href="#fnref:1:1" class="reversefootnote" role="doc-backlink">↩<sup>2</sup></a> <a href="#fnref:1:2" class="reversefootnote" role="doc-backlink">↩<sup>3</sup></a> <a href="#fnref:1:3" class="reversefootnote" role="doc-backlink">↩<sup>4</sup></a> <a href="#fnref:1:4" class="reversefootnote" role="doc-backlink">↩<sup>5</sup></a> <a href="#fnref:1:5" class="reversefootnote" role="doc-backlink">↩<sup>6</sup></a> <a href="#fnref:1:6" class="reversefootnote" role="doc-backlink">↩<sup>7</sup></a></p> </li> <li id="fn:2"> <p>Lipman Y, Chen R T Q, Ben-Hamu H, et al. Flow matching for generative modeling[J]. arXiv preprint arXiv:2210.02747, 2022. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">↩</a> <a href="#fnref:2:1" class="reversefootnote" role="doc-backlink">↩<sup>2</sup></a></p> </li> <li id="fn:3"> <p>Liu X, Gong C, Liu Q. Flow straight and fast: Learning to generate and transfer data with rectified flow[J]. arXiv preprint arXiv:2209.03003, 2022. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:5"> <p><a href="https://zhuanlan.zhihu.com/p/16026053810" rel="external nofollow noopener" target="_blank">https://zhuanlan.zhihu.com/p/16026053810</a> <a href="#fnref:5" class="reversefootnote" role="doc-backlink">↩</a> <a href="#fnref:5:1" class="reversefootnote" role="doc-backlink">↩<sup>2</sup></a> <a href="#fnref:5:2" class="reversefootnote" role="doc-backlink">↩<sup>3</sup></a></p> </li> <li id="fn:6"> <p>Gat I, Remez T, Shaul N, et al. Discrete flow matching[J]. arXiv preprint arXiv:2407.15595, 2024. <a href="#fnref:6" class="reversefootnote" role="doc-backlink">↩</a> <a href="#fnref:6:1" class="reversefootnote" role="doc-backlink">↩<sup>2</sup></a></p> </li> </ol> </div> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/%E7%A6%BB%E6%95%A3%E6%B5%81%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-2/">离散型流匹配模型（2）</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/%E7%A6%BB%E6%95%A3%E6%B5%81%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-1/">离散型流匹配模型（1）</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/%E8%BF%9E%E7%BB%AD%E6%97%B6%E9%97%B4%E7%9A%84%E9%A9%AC%E5%8F%AF%E5%A4%AB%E9%93%BE/">连续时间的马可夫链</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Yi LIU. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?c999e8c5281874b7534e3352f835d4c3" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?b977fe0c21b2118ed853308b1b923969"></script> <script src="/assets/js/no_defer.js?699fa7cbe3b29f831db7d5250ba3203a"></script> <script defer src="/assets/js/common.js?d3a25b46bbd2e0a751a27b173abc6e5f"></script> <script defer src="/assets/js/copy_code.js?fff63901a03063094790ffbfd4bc0cb4" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?25eff8ff4144a010e4ad7b31403102cf"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?04761245f225e3ad9e5e3f875d4e1074"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?b78cb42895d74e4fd6de6f633edcf181" type="text/javascript"></script> <script src="/assets/js/tabs.min.js?b3298908ffcca362ac4f3e3a7a7ebe94"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?230637657ac0b42e92d76e2b4c1b4764"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?ade73d6d60912d119f9c9347bc176630"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?f74bfa9a88ab862fb9df1c46146b7b7d"></script> </body> </html>